{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96ce675",
   "metadata": {},
   "source": [
    "# Bioinformatic analysis\n",
    "\n",
    "## 1. Preparing data for processing\n",
    "\n",
    "### 1.1 Unzipping data\n",
    "\n",
    "The sequencing data for this project came as a single zipped file (**HKeDNAworkshop2023.zip**) which we moved to the **sequenceData/2-raw** subdirectory. To check if our sequence data is in the appropriate subfolder, let us use the `ls -ltr` command again.\n",
    "\n",
    "```{code-block} bash\n",
    "ls -ltr sequenceData/2-raw\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "-rw-r--r--  1 gjeunen  staff  568074346  6 Oct 20:28 HKeDNAworkshop2023.zip\n",
    "```\n",
    "````\n",
    "\n",
    "```{warning}\n",
    "The sequencing data has the extension `.zip`. This is not the same as the extension `.gz`, which is a g-zipped file structure frequently provided by the sequencing service during file transfer. The following set of commands that deal with the `.zip` extension are specific for this data type, a result due to the alteration of files after subsetting from the original data. It is unlikely that these steps will be necessary for your project data if your data was received from a sequencing service.\n",
    "```\n",
    "\n",
    "Since the data comes zipped, we need to use the `unzip` command to get access to the sequencing files. We can use the `d` parameter to tell `unzip` where we would like to place the unzipped files. In our tutorial, we will place the files in a subdirectory of **sequenceData/2-raw/** called **sequenceData/2-raw/unzipped/**. Note that we do not need to create this subdirectory first. The `unzip` command will generate this subdirectory automatically if it not yet exists.\n",
    "\n",
    "```{code-block} bash\n",
    "unzip sequenceData/2-raw/HKeDNAworkshop2023.zip -d sequenceData/2-raw/unzipped/\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "Archive:  sequenceData/2-raw/HKeDNAworkshop2023.zip\n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK37_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK38_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK38_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK39_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK39_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK40_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK40_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK41_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK41_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK42_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK42_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK49_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK49_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK50_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK50_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK51_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK51_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK52_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK52_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK53_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK53_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK54_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_100_HK54_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK37_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK37_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK38_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK38_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK39_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK39_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK40_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK40_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK41_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK41_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK42_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK42_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK49_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK49_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK50_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK50_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK51_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK51_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK52_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK52_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK53_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK53_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK54_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/COI_500_HK54_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_1_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_1_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_2_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_2_2.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_3_1.fastq  \n",
    "  inflating: sequenceData/2-raw/unzipped/NTC_3_2.fastq  \n",
    "```\n",
    "````\n",
    "\n",
    "Unzipping the **HKeDNAworkshop2023.zip** file has generated a bunch of files for all of the samples in our experiment, each represented with a forward (**_1.fastq**) and reverse (**_2.fastq**) fastq file.\n",
    "\n",
    "#### 1.1.1 Counting files (pipe `|`)\n",
    "\n",
    "Now that we are getting more familiar with the Terminal and bash commands, let's introduce something that is called a `pipe`, which is represented by the `|` symbol. A pipe or placing the `|` in your terminal allows you to use the output from one command as the input of a second command. Hence, the terminology \"pipe\", as `|` acts as a connection between two different commands. To show you the power of piping commands together, let's look at the exercise of how you might count the number of samples you have in our data set.\n",
    "\n",
    "**STEP 1:** To solve the issue of counting the number of samples, we can use a step-by-step approach. First, we can use a command you already know, the `ls -1` command to list all the files in the directory. The `-1` parameter will print the files into a single column. Let's see the output of this command first before we continue.\n",
    "\n",
    "```{code-block} bash\n",
    "ls -1 sequenceData/2-raw/unzipped/\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "COI_100_HK37_1.fastq\n",
    "COI_100_HK37_2.fastq\n",
    "COI_100_HK38_1.fastq\n",
    "COI_100_HK38_2.fastq\n",
    "COI_100_HK39_1.fastq\n",
    "COI_100_HK39_2.fastq\n",
    "COI_100_HK40_1.fastq\n",
    "COI_100_HK40_2.fastq\n",
    "COI_100_HK41_1.fastq\n",
    "COI_100_HK41_2.fastq\n",
    "COI_100_HK42_1.fastq\n",
    "COI_100_HK42_2.fastq\n",
    "COI_100_HK49_1.fastq\n",
    "COI_100_HK49_2.fastq\n",
    "COI_100_HK50_1.fastq\n",
    "COI_100_HK50_2.fastq\n",
    "COI_100_HK51_1.fastq\n",
    "COI_100_HK51_2.fastq\n",
    "COI_100_HK52_1.fastq\n",
    "COI_100_HK52_2.fastq\n",
    "COI_100_HK53_1.fastq\n",
    "COI_100_HK53_2.fastq\n",
    "COI_100_HK54_1.fastq\n",
    "COI_100_HK54_2.fastq\n",
    "COI_500_HK37_1.fastq\n",
    "COI_500_HK37_2.fastq\n",
    "COI_500_HK38_1.fastq\n",
    "COI_500_HK38_2.fastq\n",
    "COI_500_HK39_1.fastq\n",
    "COI_500_HK39_2.fastq\n",
    "COI_500_HK40_1.fastq\n",
    "COI_500_HK40_2.fastq\n",
    "COI_500_HK41_1.fastq\n",
    "COI_500_HK41_2.fastq\n",
    "COI_500_HK42_1.fastq\n",
    "COI_500_HK42_2.fastq\n",
    "COI_500_HK49_1.fastq\n",
    "COI_500_HK49_2.fastq\n",
    "COI_500_HK50_1.fastq\n",
    "COI_500_HK50_2.fastq\n",
    "COI_500_HK51_1.fastq\n",
    "COI_500_HK51_2.fastq\n",
    "COI_500_HK52_1.fastq\n",
    "COI_500_HK52_2.fastq\n",
    "COI_500_HK53_1.fastq\n",
    "COI_500_HK53_2.fastq\n",
    "COI_500_HK54_1.fastq\n",
    "COI_500_HK54_2.fastq\n",
    "NTC_1_1.fastq\n",
    "NTC_1_2.fastq\n",
    "NTC_2_1.fastq\n",
    "NTC_2_2.fastq\n",
    "NTC_3_1.fastq\n",
    "NTC_3_2.fastq\n",
    "```\n",
    "````\n",
    "\n",
    "**STEP 2:** Once we have a list, we can use a count function such as `wc` which stands for *word count*. However, we want to count lines, not words, so we need to add the `-l` parameter to specify this. When we pipe these two commands together, the Terminal window will output the number of files in our folder. Let's try!\n",
    "\n",
    "```{code-block} bash\n",
    "ls -1 sequenceData/2-raw/unzipped/ | wc -l\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "      54\n",
    "```\n",
    "````\n",
    "\n",
    "**STEP 3:** Remember that we wanted to know the number of samples for which we have data files and that each sample was comprised of one forward (**_1.fastq**) and one reverse (**_2.fastq**) file. So, for the last step, we need to divide the number of files by 2. The easiest way to accomplis this step is to pipe the number of files in our directory to an **AWK** command (a separate language in the Terminal) that takes the number and divides it by 2. For this last command, we specify that we would like to `print` the first item (`$1`), which in our case is the number of files, and divide it by two (`/2`).\n",
    "\n",
    "```{code-block} bash\n",
    "ls -1 sequenceData/2-raw/unzipped | wc -l | awk '{print $1/2}'  \n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "27\n",
    "```\n",
    "````\n",
    "\n",
    "By using one line of code and two pipes, we have shown that we have 27 samples in our data set. In our tutorial data set, these 27 samples are comprised of 3 negative controls, and 2 size fractions \\* 3 replicates \\* 4 sites.\n",
    "\n",
    "```{tip}\n",
    "**AWK**, **sed**, and **grep** are very powerful computer languages to process text. Throughout this tutorial, we will be using snippets of code from these three computer languages. Unfortunately, the syntax of these languages is quite complex in my opinion. I recommend you to investigate these languages on your own time if bioinformatics is of interest to you. However, to keep this tutorial beginner friendly, we will not expand on the syntax further.\n",
    "```\n",
    "\n",
    "### 1.2 Fastq and Fasta file structure\n",
    "\n",
    "When unzipping the **HKeDNAworkshop2023.zip** file, we have revealed our sequence data files. Those sequence data files have the extension **.fastq**. Sequence data is most frequently represented as either **.fastq** or **.fasta** files. Both are simple text files that are structured in a particular way.\n",
    "\n",
    "For example, within **.fastq** files each sequence record is represented by 4 lines. The first line contains the header information and the line starts with the symbol `@`. The second line contains the actual sequence. The third line can contain metadata, but usually is restricted to the `+` symbol. The fourth line provides the quality of the base call and should, therefore, have the same length as the second line containing the sequence. We can inspect the file structure of one of our sequence files we just unzipped using the `head` or `tail` command. Both commands print the first or last N number of lines in a document, respectively. By using the `-n` parameter, we can specify the number of lines to be printed.\n",
    "\n",
    "```{code-block} bash\n",
    "head -n 4 sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "@M04617:136:000000000-J6LL9:1:1101:14901:1936 1:N:0:31\n",
    "GGTACTGGATGAACAGTATATCCCCCCCTAAGCTCCAATATTGCCCACGCCGGGGCGTCTGTTGACCTTGCTATCTTTAGGCTACACTTGGCTGGGGTTTCTTCTCTACTCGGGGCTGTAAACTTTATTAGAACTATCGCTAACCTGCGAGCTTTAGGGCTAATTCTTGACCGTATAACACTATTCACATGATCAGTTCTTATCACCGCCATCCTTCTCCTTCTTTCTCTACCTGTTCTCGCAGGGGCTAT\n",
    "+\n",
    "3AA@AFFFCFFFBG5GGBGBFGHCFE?AEA233FFE33D3FGBFFGBA0AEE???E?EEEAGHFB@BGGBEGDFCGGG43F3GBGBFFGFBGFECCCFDGDGHFHFHB>GFA///AD?G1<1<FGHBGH1DB1>FBGD..C../<GCF---;AC0000.:C:0CBBFFFBBB-C.C/0B0:F/FFB099C9BCF00;C0FF/BB/B..---;/.BFBFBBFFFFFFFFFF/BFFBFFFFF.9-.---9@.B\n",
    "```\n",
    "````\n",
    "\n",
    "**.fasta** files, on the other hand, are simple text files structured in a slightly different way, whereby each sequence record is represented by 2 lines. The first line contains the header information and the line starts with the symbol `>`. The second line contains the actual sequence. The third and fourth line that are in the .fastq files are missing in the .fasta files, as this file structure does not incorporate the quality of a sequence. We can inspect the file structure of a .fasta file by using the `head` or `tail` command on the reference database.\n",
    "\n",
    "```{code-block} bash\n",
    "tail -n 2 sequenceData/7-refdb/leray_COI_sintax.fasta\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    ">AB037573;tax=d:Eukaryota,p:Arthropoda,c:Insecta,o:Trichoptera,f:Limnephilidae,g:Nothopsyche,s:Nothopsyche_pallipes\n",
    "TCTTTCTAGTAATCTAGCCCACGCAGGAAGTTCAGTTGATATTTCTATTTTTTCCCTGCATTTAGCAGGAATTTCTTCAATCTTAGGGGCTATTAATTTTATCTCAACACCTTTAAATATACGAAGAAATCTAATTTCGCTAGCCCGCATTCCCCTATTTGTCTGATCGGTCGCTATCACAGCACTTCTTCTTCTTCTTTCTCTCCCCGTATTAGCTGGAGCTATCGCAATATTACTTACCGACCGTAATTTAAATACTTCCTTTTTTGATCCCTCAGGGGGCGGAGACCCCATTCTTTACCAACACTTATTT\n",
    "```\n",
    "````\n",
    "\n",
    "``````{admonition} Exercise 2\n",
    ":class: hint\n",
    "\n",
    "Knowing the file structure of the .fastq and .fasta sequence files. How would you calculate the number of sequences incorporated in the files **sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq** and **sequenceData/7-refdb/leray_COI_sintax.fasta**?\n",
    "\n",
    "``````{admonition} Answer 2\n",
    ":class: title, dropdown\n",
    "`````{tab-set}\n",
    "````{tab-item} bash Option 1\n",
    "```{code-block} bash\n",
    "grep -c \"^@M04617\" sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq\n",
    "grep -c \"^>\" sequenceData/7-refdb/leray_COI_sintax.fasta\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab-item} bash Option 2\n",
    "```{code-block} bash\n",
    "wc -l sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq\n",
    "wc -l sequenceData/7-refdb/leray_COI_sintax.fasta\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab-item} python\n",
    "```{code-block} python\n",
    "with open('sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq', 'r') as infile:\n",
    "  x = len(infile.readlines())\n",
    "  print('Total lines:', x)\n",
    "```\n",
    "````\n",
    "`````\n",
    "``````\n",
    "\n",
    "### 1.3 Changing file names\n",
    "\n",
    "When we inspect the file names of our .fastq files by using the `ls` command, we can see that they contain the `_` symbols.\n",
    "\n",
    "```{figure} filenamelist3.png\n",
    ":name: list of file names\n",
    "\n",
    ": A list of the fastq file names\n",
    "```\n",
    "\n",
    "While `_` symbols will not cause downstream incompatability issues, some symbols in file names can cause problems downstream with certain programs. One frequently observed symbol that affects one of the programs we will use during our bioinformatic pipeline is the dash `-` symbol. While not pertinent for our tutorial file names, it is important to know how to remove or change such symbols in file names in the Terminal without having do to this manually. As an exercise, let's replace all the underscores `_` to dashes `-` and back to show you how the code would work if you need it in the future.\n",
    "\n",
    "Luckily there is a simple perl one-liner to batch replace characters in file names using the `rename` command. We can tell `rename` that we want to *substitute* a pattern by providing the `s` parameter. The `/_/-/` parameter tells `rename` the pattern we want to substitute, i.e., replace `_` with `-`. Finally, we tell `rename` that we want to replace all instances using the `g` or *global* parameter. Without this parameter, `rename` would only substitute the first instance. The `*` at the end of the code tells the computer to go over all the files in the **unzipped/** directory.\n",
    "\n",
    "```{code-block} bash\n",
    "cd sequenceData/2-raw/\n",
    "rename 's/_/-/g' unzipped/*\n",
    "```\n",
    "\n",
    "```{figure} filenamelist4.png\n",
    ":name: list 4 of file names\n",
    "\n",
    ": A list of fastq file names whereby the underscores are replaced by dashes\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "rename 's/-/_/g' unzipped/*\n",
    "cd ../../\n",
    "```\n",
    "\n",
    "```{figure} filenamelist5.png\n",
    ":name: list 5 of file names\n",
    "\n",
    ": A list of fastq file names whereby the dashes are replaced back by underscores\n",
    "```\n",
    "\n",
    "### 1.4 Quality control of raw data\n",
    "\n",
    "Up to this point, we have unzipped our sequence data, checked the file structure, and batch renamed our files to exclude problematic symbols in the file names. Before processing the contents of each file, it is best to check the quality of the raw sequence data to ensure no problems were faced during sequencing. The programs we will be using to check the quality of all of our files are [FASTQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [MULTIQC](https://multiqc.info). First, we will generate reports for all of the files using FastQC. We can specify the output folder using the `-o` parameter and the number of threads or cores using the `-t` parameter.\n",
    "\n",
    "```{warning}\n",
    "**Make sure to change the value of the `-t` parameter to the number of cores available on your system!**\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "fastqc sequenceData/2-raw/unzipped/* -o sequenceData/3-fastqc/ -t 8\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "Started analysis of COI_100_HK37_1.fastq\n",
    "Approx 5% complete for COI_100_HK37_1.fastq\n",
    "Approx 10% complete for COI_100_HK37_1.fastq\n",
    "Approx 15% complete for COI_100_HK37_1.fastq\n",
    "Approx 20% complete for COI_100_HK37_1.fastq\n",
    "Approx 25% complete for COI_100_HK37_1.fastq\n",
    "Approx 30% complete for COI_100_HK37_1.fastq\n",
    "Approx 35% complete for COI_100_HK37_1.fastq\n",
    "Approx 40% complete for COI_100_HK37_1.fastq\n",
    "Approx 45% complete for COI_100_HK37_1.fastq\n",
    "Approx 50% complete for COI_100_HK37_1.fastq\n",
    "Approx 55% complete for COI_100_HK37_1.fastq\n",
    "Approx 60% complete for COI_100_HK37_1.fastq\n",
    "Approx 65% complete for COI_100_HK37_1.fastq\n",
    "Approx 70% complete for COI_100_HK37_1.fastq\n",
    "Approx 75% complete for COI_100_HK37_1.fastq\n",
    "Approx 80% complete for COI_100_HK37_1.fastq\n",
    "Approx 85% complete for COI_100_HK37_1.fastq\n",
    "Approx 90% complete for COI_100_HK37_1.fastq\n",
    "Approx 95% complete for COI_100_HK37_1.fastq\n",
    "Analysis complete for COI_100_HK37_1.fastq\n",
    "```\n",
    "````\n",
    "\n",
    "Once executed, FastQC will generate a .html report for every single file in the subdirectory **sequenceData/2-raw/unzipped/**. Let's open one report to see what it contains. From the FastQC .html reports, we are particularly interested in the **summary tab**, as well as the **per base sequence quality** and **sequence length distribution figures**.\n",
    "\n",
    "```{figure} fastqcraw.png\n",
    ":name: FastQC raw report\n",
    "\n",
    ": A FastQC report of the raw sequencing data.\n",
    "```\n",
    "\n",
    "The quality of file **COI_100_HK37_1.fastq** is looking very good and the number of sequences reported for this sample (160,732 reads) is a good sequencing depth as well!\n",
    "\n",
    "Opening each report for the 54 files separately, however, is quite tedious and makes it difficult to compare differences between files. Luckily, multiQC was developed to collate these reports into a single .html report. We can use the `.` symbol in combination with the path to specify a minimal output to the Terminal window. The `-o` parameter let's us set the output directory.\n",
    "\n",
    "```{code-block} bash\n",
    "multiqc sequenceData/3-fastqc/. -o sequenceData/3-fastq/\n",
    "```\n",
    "\n",
    "```{figure} multiqcterminal2.png\n",
    ":name: multiQC Terminal output\n",
    "\n",
    ": Terminal output for multiQC\n",
    "```\n",
    "\n",
    "The multiQC program will combine all 54 FastQC reports into a single .html document. Let's open this to see how our raw sequence data is looking like.\n",
    "\n",
    "```{figure} multiqcreport2.png\n",
    ":name: multiQC report\n",
    "\n",
    ": The .html multiQC report\n",
    "```\n",
    "\n",
    "The sequence quality, average read length, and number of sequences are all very comparable between the different files representing the samples. This is an excellent starting point! Interestingly, note the difference in length, read depth, and quality with the control samples. As those are our negative controls, these differences aren't worrysome. If, on the other hand, some samples were looking like this, you might want to consider resequencing those samples to achieve a similar sequencing depth.\n",
    "\n",
    "## 2. Merging forward and reverse reads\n",
    "\n",
    "### 2.1 A single sample example\n",
    "\n",
    "Once we have assessed the quality of the raw sequence data and did not observe any issues, we can move ahead with merging the forward and reverse reads of each sample. As a reminder, we have 54 .fastq files, indicating that we have 27 samples in our experiment, one forward and one reverse file for each sample.\n",
    "\n",
    "For this experiment and particular library, we have amplified a ~313 bp fragment of the COI gene (excluding primer-binding regions). With the sequencing run specifications of a MiSeq 2x250 paired-end V2 sequencing kit, we have a partial overlap between the forward and reverse reads in the middle of the amplicon region. This overlap is essential for successful merging. Today, we will be using the `--fastq_mergepairs` command in [VSEARCH](https://github.com/torognes/vsearch) to merge reads between the forward and reverse .fastq sequence files. We can specify the reverse sequence file through the `--reverse` parameter and the output file through the `--fastqout` parameter. To check the options available within the program, you can always read the documentation and pull up the help vignette by executing `vsearch --help` in the Terminal window. Before merging reads immediately on all samples, it is best to play around with the parameters on a single sample.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --fastq_mergepairs sequenceData/2-raw/unzipped/COI_100_HK37_1.fastq --reverse sequenceData/2-raw/unzipped/COI_100_HK37_2.fastq --fastqout sequenceData/2-raw/COI_100_HK37merged.fastq\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    160732  Pairs\n",
    "    146040  Merged (90.9%)\n",
    "     14692  Not merged (9.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       144  too few kmers found on same diagonal\n",
    "      5751  too many differences\n",
    "      8584  alignment score too low, or score drop too high\n",
    "       213  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    249.48  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    359.43  Mean fragment length\n",
    "     27.01  Standard deviation of fragment length\n",
    "      0.32  Mean expected error in forward sequences\n",
    "      0.53  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.22  Mean observed errors in merged region of forward sequences\n",
    "      0.56  Mean observed errors in merged region of reverse sequences\n",
    "      0.77  Mean observed errors in merged region\n",
    "```\n",
    "````\n",
    "\n",
    "Using the default settings, we managed to merge 90.9% of reads. For this tutorial, we'll take that as sufficient. However, for your own project, you might want to explore the various options within the `fastq_mergepairs` command further to try and increase the percentage of reads merged. From the `vsearch --fastq_mergepairs` output that was printed to the Terminal window, we can see that VSEARCH reports the number of observed pairs in the raw data files, the number of reads successfully merged, and the number of reads unable to be merged. Additionally, information on why reads couldn't be merged and statistics about merged reads are provided.\n",
    "\n",
    "### 2.2 Batch merging reads\n",
    "\n",
    "Since we need to merge sequences for 27 samples, we will introduce a new coding concept, the **for loop**. A for loop allows us to move through a list and execute a command on each item. While we will not go into too much detail for this tutorial, the basic syntax of for loops is as follows:\n",
    "\n",
    "```{code-block} bash\n",
    "for file in list\n",
    "do\n",
    "  execute command\n",
    "done\n",
    "```\n",
    "\n",
    "Basically, the first line tells us that the program will go through a list and `for` each item in the list, `do` something (line 2), which in this case is `execute command` (line 3). Once the program has executed the command in all items within the list, we need to close the loop by specifying `done` (line 4). So, with a for loop, we can tell the computer to merge reads for our 27 samples automatically, saving us the time to have to execute the code to merge reads 27 times by ourselves.\n",
    "\n",
    "Within our for loop to merge reads for all samples, we will generate a list of all forward sequencing files (`*_1.fastq`). Before writing the code to merge reads in our for loop, we will first print which sample is being merged, as the VSEARCH output does not specify this (`echo \"Merging reads for: ${R1/_1.fastq/}\"`). To print to the Terminal, we can use the `echo` command. With the last parameter for the `echo` command (`${R1/_1.fastq/}\"`), we specify that from our forward sequencing file name `$R1`, substitute `_1.fastq` with nothing. After the `echo` command, we execute the `vsearch --fastq_mergepairs` command as we did above, but we need to use the for loop syntax to tell the program what our forward, reverse, and output file names are. Remember that those names change when the program iterates over the list, so we cannot hard code them into the command.\n",
    "\n",
    "```{code-block} bash\n",
    "rm sequenceData/2-raw/COI_100_HK37merged.fastq \n",
    "cd sequenceData/2-raw/unzipped/\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "for R1 in *_1.fastq\n",
    "do\n",
    "\n",
    "  echo \"\\n\\nMerging reads for: ${R1/_1.fastq/}\"\n",
    "  vsearch --fastq_mergepairs ${R1} --reverse ${R1/_1.fastq/_2.fastq} --fastqout ../${R1/1.fastq/merged.fastq}\n",
    "\n",
    "done\n",
    "\n",
    "cd ../../../\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "Merging reads for: COI_100_HK37\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    160732  Pairs\n",
    "    146040  Merged (90.9%)\n",
    "     14692  Not merged (9.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       144  too few kmers found on same diagonal\n",
    "      5751  too many differences\n",
    "      8584  alignment score too low, or score drop too high\n",
    "       213  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    249.48  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    359.43  Mean fragment length\n",
    "     27.01  Standard deviation of fragment length\n",
    "      0.32  Mean expected error in forward sequences\n",
    "      0.53  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.22  Mean observed errors in merged region of forward sequences\n",
    "      0.56  Mean observed errors in merged region of reverse sequences\n",
    "      0.77  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK38\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    129473  Pairs\n",
    "    118917  Merged (91.8%)\n",
    "     10556  Not merged (8.2%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       116  too few kmers found on same diagonal\n",
    "      4920  too many differences\n",
    "      5434  alignment score too low, or score drop too high\n",
    "        86  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    249.92  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    361.90  Mean fragment length\n",
    "     20.07  Standard deviation of fragment length\n",
    "      0.30  Mean expected error in forward sequences\n",
    "      0.54  Mean expected error in reverse sequences\n",
    "      0.25  Mean expected error in merged sequences\n",
    "      0.21  Mean observed errors in merged region of forward sequences\n",
    "      0.58  Mean observed errors in merged region of reverse sequences\n",
    "      0.79  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK39\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    181198  Pairs\n",
    "    162326  Merged (89.6%)\n",
    "     18872  Not merged (10.4%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       137  too few kmers found on same diagonal\n",
    "         2  multiple potential alignments\n",
    "      6778  too many differences\n",
    "     11862  alignment score too low, or score drop too high\n",
    "         1  overlap too short\n",
    "        92  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.08  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    362.18  Mean fragment length\n",
    "     18.32  Standard deviation of fragment length\n",
    "      0.33  Mean expected error in forward sequences\n",
    "      0.48  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.23  Mean observed errors in merged region of forward sequences\n",
    "      0.50  Mean observed errors in merged region of reverse sequences\n",
    "      0.73  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK40\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    130928  Pairs\n",
    "    115722  Merged (88.4%)\n",
    "     15206  Not merged (11.6%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       144  too few kmers found on same diagonal\n",
    "      4604  too many differences\n",
    "     10378  alignment score too low, or score drop too high\n",
    "        80  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.12  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    362.11  Mean fragment length\n",
    "     19.02  Standard deviation of fragment length\n",
    "      0.30  Mean expected error in forward sequences\n",
    "      0.49  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.23  Mean observed errors in merged region of forward sequences\n",
    "      0.52  Mean observed errors in merged region of reverse sequences\n",
    "      0.75  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK41\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    115679  Pairs\n",
    "    102320  Merged (88.5%)\n",
    "     13359  Not merged (11.5%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       108  too few kmers found on same diagonal\n",
    "         2  multiple potential alignments\n",
    "      4047  too many differences\n",
    "      9067  alignment score too low, or score drop too high\n",
    "       135  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.07  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    361.47  Mean fragment length\n",
    "     20.81  Standard deviation of fragment length\n",
    "      0.28  Mean expected error in forward sequences\n",
    "      0.54  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.20  Mean observed errors in merged region of forward sequences\n",
    "      0.59  Mean observed errors in merged region of reverse sequences\n",
    "      0.79  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK42\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    164258  Pairs\n",
    "    148463  Merged (90.4%)\n",
    "     15795  Not merged (9.6%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       112  too few kmers found on same diagonal\n",
    "         1  multiple potential alignments\n",
    "      5409  too many differences\n",
    "     10244  alignment score too low, or score drop too high\n",
    "        29  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.39  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    363.83  Mean fragment length\n",
    "     11.18  Standard deviation of fragment length\n",
    "      0.29  Mean expected error in forward sequences\n",
    "      0.48  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.21  Mean observed errors in merged region of forward sequences\n",
    "      0.51  Mean observed errors in merged region of reverse sequences\n",
    "      0.72  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK49\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100%  \n",
    "     97006  Pairs\n",
    "     91171  Merged (94.0%)\n",
    "      5835  Not merged (6.0%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        51  too few kmers found on same diagonal\n",
    "      2739  too many differences\n",
    "      3039  alignment score too low, or score drop too high\n",
    "         6  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.31  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.45  Mean fragment length\n",
    "     10.51  Standard deviation of fragment length\n",
    "      0.24  Mean expected error in forward sequences\n",
    "      0.49  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.18  Mean observed errors in merged region of forward sequences\n",
    "      0.54  Mean observed errors in merged region of reverse sequences\n",
    "      0.72  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK50\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     85963  Pairs\n",
    "     80114  Merged (93.2%)\n",
    "      5849  Not merged (6.8%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        48  too few kmers found on same diagonal\n",
    "      2669  too many differences\n",
    "      3127  alignment score too low, or score drop too high\n",
    "         5  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.46  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.77  Mean fragment length\n",
    "      6.27  Standard deviation of fragment length\n",
    "      0.24  Mean expected error in forward sequences\n",
    "      0.50  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.18  Mean observed errors in merged region of forward sequences\n",
    "      0.54  Mean observed errors in merged region of reverse sequences\n",
    "      0.72  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK51\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     89943  Pairs\n",
    "     83923  Merged (93.3%)\n",
    "      6020  Not merged (6.7%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        41  too few kmers found on same diagonal\n",
    "      2547  too many differences\n",
    "      3423  alignment score too low, or score drop too high\n",
    "         9  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.36  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.57  Mean fragment length\n",
    "      9.74  Standard deviation of fragment length\n",
    "      0.26  Mean expected error in forward sequences\n",
    "      0.52  Mean expected error in reverse sequences\n",
    "      0.25  Mean expected error in merged sequences\n",
    "      0.19  Mean observed errors in merged region of forward sequences\n",
    "      0.57  Mean observed errors in merged region of reverse sequences\n",
    "      0.76  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK52\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     80397  Pairs\n",
    "     74255  Merged (92.4%)\n",
    "      6142  Not merged (7.6%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        31  too few kmers found on same diagonal\n",
    "      2511  too many differences\n",
    "      3597  alignment score too low, or score drop too high\n",
    "         3  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.38  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.59  Mean fragment length\n",
    "      8.22  Standard deviation of fragment length\n",
    "      0.27  Mean expected error in forward sequences\n",
    "      0.44  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.22  Mean observed errors in merged region of forward sequences\n",
    "      0.46  Mean observed errors in merged region of reverse sequences\n",
    "      0.69  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK53\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     82962  Pairs\n",
    "     76890  Merged (92.7%)\n",
    "      6072  Not merged (7.3%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        34  too few kmers found on same diagonal\n",
    "      2217  too many differences\n",
    "      3815  alignment score too low, or score drop too high\n",
    "         6  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.37  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.52  Mean fragment length\n",
    "      8.68  Standard deviation of fragment length\n",
    "      0.26  Mean expected error in forward sequences\n",
    "      0.43  Mean expected error in reverse sequences\n",
    "      0.22  Mean expected error in merged sequences\n",
    "      0.19  Mean observed errors in merged region of forward sequences\n",
    "      0.46  Mean observed errors in merged region of reverse sequences\n",
    "      0.65  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_100_HK54\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     81013  Pairs\n",
    "     75723  Merged (93.5%)\n",
    "      5290  Not merged (6.5%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        38  too few kmers found on same diagonal\n",
    "      2445  too many differences\n",
    "      2802  alignment score too low, or score drop too high\n",
    "         5  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.41  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.70  Mean fragment length\n",
    "      7.32  Standard deviation of fragment length\n",
    "      0.24  Mean expected error in forward sequences\n",
    "      0.46  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.18  Mean observed errors in merged region of forward sequences\n",
    "      0.50  Mean observed errors in merged region of reverse sequences\n",
    "      0.68  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK37\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    120585  Pairs\n",
    "    109424  Merged (90.7%)\n",
    "     11161  Not merged (9.3%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       111  too few kmers found on same diagonal\n",
    "      4412  too many differences\n",
    "      6635  alignment score too low, or score drop too high\n",
    "         3  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.44  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.34  Mean fragment length\n",
    "      4.63  Standard deviation of fragment length\n",
    "      0.33  Mean expected error in forward sequences\n",
    "      0.47  Mean expected error in reverse sequences\n",
    "      0.25  Mean expected error in merged sequences\n",
    "      0.25  Mean observed errors in merged region of forward sequences\n",
    "      0.49  Mean observed errors in merged region of reverse sequences\n",
    "      0.74  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK38\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     93778  Pairs\n",
    "     86827  Merged (92.6%)\n",
    "      6951  Not merged (7.4%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        88  too few kmers found on same diagonal\n",
    "      2664  too many differences\n",
    "      4191  alignment score too low, or score drop too high\n",
    "         8  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.52  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.40  Mean fragment length\n",
    "      4.26  Standard deviation of fragment length\n",
    "      0.30  Mean expected error in forward sequences\n",
    "      0.48  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.21  Mean observed errors in merged region of forward sequences\n",
    "      0.49  Mean observed errors in merged region of reverse sequences\n",
    "      0.71  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK39\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    146550  Pairs\n",
    "    134723  Merged (91.9%)\n",
    "     11827  Not merged (8.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       156  too few kmers found on same diagonal\n",
    "         1  multiple potential alignments\n",
    "      4664  too many differences\n",
    "      6996  alignment score too low, or score drop too high\n",
    "        10  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.45  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.09  Mean fragment length\n",
    "      8.67  Standard deviation of fragment length\n",
    "      0.32  Mean expected error in forward sequences\n",
    "      0.45  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.24  Mean observed errors in merged region of forward sequences\n",
    "      0.47  Mean observed errors in merged region of reverse sequences\n",
    "      0.71  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK40\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    119622  Pairs\n",
    "    109897  Merged (91.9%)\n",
    "      9725  Not merged (8.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       128  too few kmers found on same diagonal\n",
    "      3864  too many differences\n",
    "      5728  alignment score too low, or score drop too high\n",
    "         5  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.48  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.28  Mean fragment length\n",
    "      5.43  Standard deviation of fragment length\n",
    "      0.31  Mean expected error in forward sequences\n",
    "      0.49  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.23  Mean observed errors in merged region of forward sequences\n",
    "      0.50  Mean observed errors in merged region of reverse sequences\n",
    "      0.73  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK41\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    128094  Pairs\n",
    "    117560  Merged (91.8%)\n",
    "     10534  Not merged (8.2%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       179  too few kmers found on same diagonal\n",
    "      4244  too many differences\n",
    "      6104  alignment score too low, or score drop too high\n",
    "         7  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.51  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.37  Mean fragment length\n",
    "      6.25  Standard deviation of fragment length\n",
    "      0.27  Mean expected error in forward sequences\n",
    "      0.48  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.19  Mean observed errors in merged region of forward sequences\n",
    "      0.54  Mean observed errors in merged region of reverse sequences\n",
    "      0.72  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK42\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     84467  Pairs\n",
    "     77020  Merged (91.2%)\n",
    "      7447  Not merged (8.8%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        69  too few kmers found on same diagonal\n",
    "         1  multiple potential alignments\n",
    "      3141  too many differences\n",
    "      4233  alignment score too low, or score drop too high\n",
    "         3  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.61  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.63  Mean fragment length\n",
    "      6.61  Standard deviation of fragment length\n",
    "      0.24  Mean expected error in forward sequences\n",
    "      0.53  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.17  Mean observed errors in merged region of forward sequences\n",
    "      0.64  Mean observed errors in merged region of reverse sequences\n",
    "      0.80  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK49\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     74265  Pairs\n",
    "     67718  Merged (91.2%)\n",
    "      6547  Not merged (8.8%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        81  too few kmers found on same diagonal\n",
    "      2989  too many differences\n",
    "      3473  alignment score too low, or score drop too high\n",
    "         4  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.47  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.75  Mean fragment length\n",
    "      6.74  Standard deviation of fragment length\n",
    "      0.24  Mean expected error in forward sequences\n",
    "      0.56  Mean expected error in reverse sequences\n",
    "      0.26  Mean expected error in merged sequences\n",
    "      0.17  Mean observed errors in merged region of forward sequences\n",
    "      0.62  Mean observed errors in merged region of reverse sequences\n",
    "      0.79  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK50\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     81354  Pairs\n",
    "     75588  Merged (92.9%)\n",
    "      5766  Not merged (7.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        81  too few kmers found on same diagonal\n",
    "      2482  too many differences\n",
    "      3199  alignment score too low, or score drop too high\n",
    "         4  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.52  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.66  Mean fragment length\n",
    "      7.57  Standard deviation of fragment length\n",
    "      0.27  Mean expected error in forward sequences\n",
    "      0.47  Mean expected error in reverse sequences\n",
    "      0.23  Mean expected error in merged sequences\n",
    "      0.20  Mean observed errors in merged region of forward sequences\n",
    "      0.49  Mean observed errors in merged region of reverse sequences\n",
    "      0.70  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK51\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    113932  Pairs\n",
    "    105503  Merged (92.6%)\n",
    "      8429  Not merged (7.4%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       110  too few kmers found on same diagonal\n",
    "      3809  too many differences\n",
    "      4505  alignment score too low, or score drop too high\n",
    "         5  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.54  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.66  Mean fragment length\n",
    "      7.14  Standard deviation of fragment length\n",
    "      0.26  Mean expected error in forward sequences\n",
    "      0.52  Mean expected error in reverse sequences\n",
    "      0.24  Mean expected error in merged sequences\n",
    "      0.20  Mean observed errors in merged region of forward sequences\n",
    "      0.54  Mean observed errors in merged region of reverse sequences\n",
    "      0.74  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK52\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    121246  Pairs\n",
    "    113930  Merged (94.0%)\n",
    "      7316  Not merged (6.0%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        91  too few kmers found on same diagonal\n",
    "      2602  too many differences\n",
    "      4619  alignment score too low, or score drop too high\n",
    "         4  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.43  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.74  Mean fragment length\n",
    "      6.49  Standard deviation of fragment length\n",
    "      0.21  Mean expected error in forward sequences\n",
    "      0.44  Mean expected error in reverse sequences\n",
    "      0.20  Mean expected error in merged sequences\n",
    "      0.15  Mean observed errors in merged region of forward sequences\n",
    "      0.46  Mean observed errors in merged region of reverse sequences\n",
    "      0.61  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK53\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    108046  Pairs\n",
    "    100604  Merged (93.1%)\n",
    "      7442  Not merged (6.9%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       104  too few kmers found on same diagonal\n",
    "      2833  too many differences\n",
    "      4502  alignment score too low, or score drop too high\n",
    "         3  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.43  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.62  Mean fragment length\n",
    "      6.46  Standard deviation of fragment length\n",
    "      0.23  Mean expected error in forward sequences\n",
    "      0.43  Mean expected error in reverse sequences\n",
    "      0.22  Mean expected error in merged sequences\n",
    "      0.18  Mean observed errors in merged region of forward sequences\n",
    "      0.47  Mean observed errors in merged region of reverse sequences\n",
    "      0.64  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: COI_500_HK54\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "    104984  Pairs\n",
    "     95343  Merged (90.8%)\n",
    "      9641  Not merged (9.2%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        74  too few kmers found on same diagonal\n",
    "      2427  too many differences\n",
    "      7134  alignment score too low, or score drop too high\n",
    "         6  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    250.33  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    364.39  Mean fragment length\n",
    "     10.29  Standard deviation of fragment length\n",
    "      0.23  Mean expected error in forward sequences\n",
    "      0.44  Mean expected error in reverse sequences\n",
    "      0.21  Mean expected error in merged sequences\n",
    "      0.17  Mean observed errors in merged region of forward sequences\n",
    "      0.46  Mean observed errors in merged region of reverse sequences\n",
    "      0.63  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: NTC_1\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "       960  Pairs\n",
    "       306  Merged (31.9%)\n",
    "       654  Not merged (68.1%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "        13  too few kmers found on same diagonal\n",
    "       134  too many differences\n",
    "       501  alignment score too low, or score drop too high\n",
    "         6  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "    233.35  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "    287.81  Mean fragment length\n",
    "    132.89  Standard deviation of fragment length\n",
    "      0.49  Mean expected error in forward sequences\n",
    "      1.03  Mean expected error in reverse sequences\n",
    "      0.63  Mean expected error in merged sequences\n",
    "      0.56  Mean observed errors in merged region of forward sequences\n",
    "      1.49  Mean observed errors in merged region of reverse sequences\n",
    "      2.05  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: NTC_2\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "     10408  Pairs\n",
    "      7842  Merged (75.3%)\n",
    "      2566  Not merged (24.7%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "      1590  too few kmers found on same diagonal\n",
    "        15  multiple potential alignments\n",
    "       109  too many differences\n",
    "       574  alignment score too low, or score drop too high\n",
    "       278  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "     80.31  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "     64.72  Mean fragment length\n",
    "     42.73  Standard deviation of fragment length\n",
    "      0.09  Mean expected error in forward sequences\n",
    "      0.11  Mean expected error in reverse sequences\n",
    "      0.05  Mean expected error in merged sequences\n",
    "      0.18  Mean observed errors in merged region of forward sequences\n",
    "      0.25  Mean observed errors in merged region of reverse sequences\n",
    "      0.43  Mean observed errors in merged region\n",
    "\n",
    "\n",
    "Merging reads for: NTC_3\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Merging reads 100% \n",
    "      7779  Pairs\n",
    "      6114  Merged (78.6%)\n",
    "      1665  Not merged (21.4%)\n",
    "\n",
    "Pairs that failed merging due to various reasons:\n",
    "       413  too few kmers found on same diagonal\n",
    "        10  multiple potential alignments\n",
    "       162  too many differences\n",
    "       819  alignment score too low, or score drop too high\n",
    "       261  staggered read pairs\n",
    "\n",
    "Statistics of all reads:\n",
    "     93.17  Mean read length\n",
    "\n",
    "Statistics of merged reads:\n",
    "     72.01  Mean fragment length\n",
    "     52.72  Standard deviation of fragment length\n",
    "      0.12  Mean expected error in forward sequences\n",
    "      0.14  Mean expected error in reverse sequences\n",
    "      0.09  Mean expected error in merged sequences\n",
    "      0.28  Mean observed errors in merged region of forward sequences\n",
    "      0.30  Mean observed errors in merged region of reverse sequences\n",
    "      0.58  Mean observed errors in merged region\n",
    "```\n",
    "````\n",
    "\n",
    "When executing this for loop, VSEARCH will attempt to merge all reads from the forward and reverse sequencing file for each sample separately. The output files containing the merged reads will be placed in the subdirectory **sequenceData/2-raw/** and will have the extension **_merged.fastq** rather than **_1.fastq** or **_2.fastq**. Within the Terminal window, all the statistics are reported for each sample separately. A quick glance shows us that we managed to merge roughly 90% of reads for each sample. However, due to the number of samples, it is difficult to get an overview of the read merging success rate and compare different samples.\n",
    "\n",
    "### 2.3 Summarizing the output\n",
    "\n",
    "To get a better overview, we can use the following python script to generate a bar plot with the raw and merged read statistics. The python code takes in two arguments, the first is the location where the merged sequence files are stored, the second is the location of the raw sequence files. Since this is python code, we cannot directly copy-paste the code into the Terminal as we did before. Rather, we have to create our first script. To do this, we will use a text editor that is implemented in the Terminal called `nano`. Running `nano` will open a new text window where we can copy-paste our python code. For this tutorial, it is not important or essential to understand python, though it is an extremely powerful and useful coding language for bioinforamticians!. To provide some context, I'll give a brief explanation during the workshop when we're executing this code.\n",
    "\n",
    "```{code-block} bash\n",
    "nano sequenceData/1-scripts/rawMergedStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "## import modules\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## user arguments\n",
    "mergedPath = sys.argv[1]\n",
    "rawPath = sys.argv[2]\n",
    "\n",
    "## first, create a sample name list\n",
    "mergedFileList = os.listdir(mergedPath)\n",
    "sampleNameList = []\n",
    "for mergedFile in mergedFileList:\n",
    "    sampleName = mergedFile.split('_merged.fastq')[0]\n",
    "    if sampleName.startswith('COI_') or sampleName.startswith('NTC_'):\n",
    "        sampleNameList.append(sampleName)\n",
    "\n",
    "## count number of raw and merged sequences for each sample in sampleNameList\n",
    "rawSeqCount = {}\n",
    "mergedSeqCount = {}\n",
    "for sample in sampleNameList:\n",
    "    with open(f'{mergedPath}{sample}_merged.fastq', 'r') as mergedFile:\n",
    "        x = len(mergedFile.readlines()) / 4\n",
    "        mergedSeqCount[sample] = int(x)\n",
    "    with open(f'{rawPath}{sample}_1.fastq', 'r') as rawFile:\n",
    "        y = len(rawFile.readlines()) / 4\n",
    "        rawSeqCount[sample] = int(y)\n",
    "\n",
    "## create a dataframe from the dictionaries\n",
    "df = pd.DataFrame({'Sample': list(rawSeqCount.keys()), 'Raw': list(rawSeqCount.values()), 'Merged': list(mergedSeqCount.values())})\n",
    "\n",
    "## sort the dataframe by raw reads in descending order\n",
    "df = df.sort_values(by='Raw', ascending=False)\n",
    "\n",
    "## calculate the percentage of merged/raw and format it with 2 decimal places and the '%' symbol\n",
    "df['Percentage'] = (df['Merged'] / df['Raw'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "## create a horizontal bar plot using seaborn\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
    "\n",
    "## use seaborn's barplot\n",
    "ax = sns.barplot(x='Raw', y='Sample', data=df, label='Raw', color='#BBC6C8')\n",
    "sns.barplot(x='Merged', y='Sample', data=df, label='Merged', color='#469597')\n",
    "\n",
    "## add labels and title\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Horizontal bar graph of raw and merged reads (Sorted by Total in Reverse)')\n",
    "\n",
    "## add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add raw read count next to the bars\n",
    "for i, v in enumerate(df['Percentage']):\n",
    "    ax.text(df['Raw'].values[i] + 50, i, v, va='center', fontsize=10, color='black')\n",
    "\n",
    "## save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('raw_and_merged_bargraph.png', dpi = 300)\n",
    "```\n",
    "\n",
    "Once we have copy-pasted the code, we can press `ctrl +x` to exit out of the editor, followed by `y` and `return` to save the file. After doing so, we're back in the normal Terminal window. Before we can run or execute our first script, we need to make it executable.\n",
    "\n",
    "`````{important}\n",
    "Remember the permissions for each file that we discussed before? By running the `ls -ltr` command, we can see that for the script we have just created, we only have read (`r`) and write (`w`) access, but no execution (`x`) permission.\n",
    "\n",
    "```{code-block} bash\n",
    "ls -ltr sequenceData/1-scripts/\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "-rw-r--r--  1 gjeunen  staff  2149  6 Oct 22:56 rawMergedStatistics.py\n",
    "```\n",
    "````\n",
    "To change the permissions or modifiers of a file, we can use the `chmod` command, which stands for *change modifier*. Since we want to make our file executable, we can specify the parameter `+x`.\n",
    "`````\n",
    "\n",
    "```{code-block} bash\n",
    "chmod +x sequenceData/1-scripts/rawMergedStatistics.py\n",
    "```\n",
    "\n",
    "Rerunning the `ls -ltr` command shows that we have changed the permissions and that we can execute our script.\n",
    "\n",
    "```{code-block} bash\n",
    "ls -ltr sequenceData/1-scripts/\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "-rwxr-xr-x  1 gjeunen  staff  2149  6 Oct 22:56 rawMergedStatistics.py\n",
    "```\n",
    "````\n",
    "\n",
    "To execute the script, we can use the `./` command followed by the python script and our two user parameters where our (*parameter 1*) merged and (*parameter 2*) raw files are located.\n",
    "\n",
    "```{code-block} bash\n",
    "./sequenceData/1-scripts/rawMergedStatistics.py sequenceData/2-raw/ sequenceData/2-raw/unzipped/\n",
    "```\n",
    "\n",
    "```{figure} raw_and_merged_bargraph.png\n",
    ":name: Raw and merged read count\n",
    "\n",
    ": Raw and merged read count\n",
    "```\n",
    "\n",
    "The figure we generated using the python script shows a horizontal bar graph with the merged reads in green and the raw sequence counts in grey. We can see that the percentage of merged reads is similar across all the samples, roughly 90%. We can also see that there is some abundance differences between the samples, which is to be expected, as pooling samples equimolarly is hard to do in the lab, something we covered during the first week of this eDNA workshop. Also, note the big difference between the samples and the negative controls, something we observed earlier in the [MULTIQC](https://multiqc.info) report as well.\n",
    "\n",
    "````{admonition} Amplicon length exceeding cycle number\n",
    ":class: tip\n",
    "To merge forward and reverse reads, it is essential to have at least a partial overlap in the amplicon region with the forward and reverse reads. Even better would be a full overlap, as it increases the base call quality score. Sometimes, however, the amplicon length is too long to achieve partial overlap with paired-end reads on an Illumina platform, since the Illumina sequencing technology is restricted by sequence length. While such a scenario should be avoided at all cost, if this was a situation you would find yourself in, you can concatenate paired-end reads using the following line of code, rather than merge them.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --fastq_join input_R1.fastq --reverse input_R2.fastq --fastqout output_joined.fastq\n",
    "```\n",
    "\n",
    "Keep in mind 3rd generation sequencing technologies, which we discussed during the first week of this workshop, if your experiment would benefit from using longer amplicon lengths than can be covered by Illumina sequencing technology.\n",
    "````\n",
    "\n",
    "## 3. Removing primer sequences\n",
    "\n",
    "### 3.1 A single sample example\n",
    "\n",
    "At this point, our merged reads still contain the primer sequences. Since these regions are artefacts from the PCR amplification and not biological, they will need to be removed from the reads before continuing with the bioinformatic pipeline. For this library and experiment, we have used the mlCOIintF/jgHCO2198 primer set (Leray et al., 2013). The forward primer corresponds to 5'-GGWACWGGWTGAACWGTWTAYCCYCC-3' and the reverse primer sequence is 5'-TAIACYTCIGGRTGICCRAARAAYCA-3'. Before batch processing every sample, let's test our code on a single sample again to start with. For primer or adapter removal, we can use the program [cutadapt](https://cutadapt.readthedocs.io/en/stable/). To specify the primers to be removed, we can use the `-a` parameter. Since we're removing both the forward and reverse primer, we can link them together using `...`. Remember to use the reverse complement of the reverse primer, as this would be the direction the reverse primer is found in our sequence data after merging. The minimum and maximum length of the amplicon can be specified with the `-m` and `-M` parameters, respectively. To only keep reads for which both primers were found and removed, we need to specify the `--discard-untrimmed` option. The `--no-indels` and `-e 2` parameters allow us to tell the program to not include insertions and deletions in the search and allow a maximum of 2 errors in the primer sequence. We can specify the `--revcomp` parameter to search for the primer sequences in both directions. Finally, we can use the `--cores=0` parameter to automatically detect the number of available cores.\n",
    "\n",
    "```{code-block} bash\n",
    "cutadapt sequenceData/2-raw/COI_100_HK37_merged.fastq -a GGWACWGGWTGAACWGTWTAYCCYCC...TGRTTYTTYGGHCAYCCHGARGTHTA -m 300 -M 330 --discard-untrimmed -o sequenceData/4-demux/COI_100_HK37_trimmed.fastq --no-indels -e 2 --revcomp --cores=0\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "This is cutadapt 4.4 with Python 3.11.5\n",
    "Command line parameters: sequenceData/2-raw/COI_100_HK37_merged.fastq -a GGWACWGGWTGAACWGTWTAYCCYCC...TGRTTYTTYGGHCAYCCHGARGTHTA -m 300 -M 330 --discard-untrimmed -o sequenceData/4-demux/COI_100_HK37_trimmed.fastq --no-indels -e 2 --revcomp --cores=0\n",
    "Processing single-end reads on 8 cores ...\n",
    "Done           00:00:00       146,040 reads @   4.1 µs/read;  14.69 M reads/minute\n",
    "Finished in 0.598 s (4.093 µs/read; 14.66 M reads/minute).\n",
    "\n",
    "=== Summary ===\n",
    "\n",
    "Total reads processed:                 146,040\n",
    "Reads with adapters:                   145,999 (100.0%)\n",
    "Reverse-complemented:                        4 (0.0%)\n",
    "\n",
    "== Read fate breakdown ==\n",
    "Reads that were too short:               4,877 (3.3%)\n",
    "Reads that were too long:                4,754 (3.3%)\n",
    "Reads discarded as untrimmed:                0 (0.0%)\n",
    "Reads written (passing filters):       136,409 (93.4%)\n",
    "\n",
    "Total basepairs processed:    52,491,397 bp\n",
    "Total written (filtered):     42,623,291 bp (81.2%)\n",
    "\n",
    "=== Adapter 1 ===\n",
    "\n",
    "Sequence: GGWACWGGWTGAACWGTWTAYCCYCC...TGRTTYTTYGGHCAYCCHGARGTHTA; Type: linked; Length: 26+26; 5' trimmed: 143311 times; 3' trimmed: 143808 times; Reverse-complemented: 4 times\n",
    "\n",
    "Minimum overlap: 3+3\n",
    "No. of allowed errors:\n",
    "1-12 bp: 0; 13-25 bp: 1; 26 bp: 2\n",
    "\n",
    "No. of allowed errors:\n",
    "1-12 bp: 0; 13-25 bp: 1; 26 bp: 2\n",
    "\n",
    "Overview of removed sequences at 5' end\n",
    "length count expect max.err error counts\n",
    "4 2 570.5 0 2\n",
    "5 1 142.6 0 1\n",
    "11 1 0.0 0 1\n",
    "13 1 0.0 1 1\n",
    "14 1 0.0 1 0 1\n",
    "15 6 0.0 1 6\n",
    "16 5 0.0 1 4 1\n",
    "17 10 0.0 1 2 8\n",
    "18 5 0.0 1 4 1\n",
    "19 20 0.0 1 14 6\n",
    "20 13 0.0 1 11 2\n",
    "21 15 0.0 1 6 9\n",
    "22 12 0.0 1 10 2\n",
    "23 10 0.0 1 9 1\n",
    "24 31 0.0 1 14 17\n",
    "25 851 0.0 1 552 299\n",
    "26 142152 0.0 2 140479 1461 212\n",
    "27 175 0.0 2 142 27 6\n",
    "\n",
    "Overview of removed sequences at 3' end\n",
    "length count expect max.err error counts\n",
    "13 1 0.0 1 0 1\n",
    "14 2 0.0 1 2\n",
    "15 1 0.0 1 1\n",
    "16 5 0.0 1 2 3\n",
    "17 3 0.0 1 0 3\n",
    "18 10 0.0 1 4 6\n",
    "19 3 0.0 1 0 3\n",
    "20 6 0.0 1 4 2\n",
    "21 14 0.0 1 9 5\n",
    "22 19 0.0 1 15 4\n",
    "23 65 0.0 1 48 17\n",
    "24 77 0.0 1 62 15\n",
    "25 174 0.0 1 45 129\n",
    "26 142067 0.0 2 140032 1686 349\n",
    "27 1351 0.0 2 749 370 232\n",
    "28 8 0.0 2 3 2 3\n",
    "63 1 0.0 2 0 0 1\n",
    "70 1 0.0 2 0 0 1\n",
    "```\n",
    "````\n",
    "\n",
    "The cutadapt output provides information on how many reads were analysed, how many reads were trimmed on the plus and minus strand, plus a detailed overview of where the adapters were cut in the sequence. For our sample **sequenceData/2-raw/COI_100_HK37_merged.fastq**, 146,040 reads were processed, 145,999 reads were found to contain both primers, and 4 reads were found where the primers were present as the reverse complement of the read. As these options that we specified in our command managed to remove the primer sequences from nearly all reads, we will use these settings in our for loop to batch process all samples.\n",
    "\n",
    "### 3.2 batch trimming\n",
    "\n",
    "Similarly to the merging of reads, we need to process 27 samples. Hence, we will write another for loop to accomplish this goal, rather than executing the code to trim primer sequences 27 times by ourselves. Because the cutadapt output that is written to the Terminal window is very elaborate, we will write it to a file instead. In the next section (*Summarizing cutadapt output*), we'll parse the output to produce some graphs in order to get a better overview of how cutadapt performed across the 27 samples. First, we will remove the file we just generated and move to the **sequenceData/2-raw/** directory. As we've introduced the for loop syntax before and explained the cutadapt code snippet, I won't be going into too much detail on each aspect.\n",
    "\n",
    "```{code-block} bash\n",
    "rm sequenceData/4-demux/COI_100_HK37_trimmed.fastq\n",
    "cd sequenceData/2-raw/\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "for fq in *merged.fastq\n",
    "do\n",
    "\n",
    "  echo \"trimming primer seqs for: ${fq/_merged.fastq/}\"\n",
    "  cutadapt ${fq} -a GGWACWGGWTGAACWGTWTAYCCYCC...TGRTTYTTYGGHCAYCCHGARGTHTA -m 300 -M 330 --discard-untrimmed -o ../4-demux/${fq/merged.fastq/trimmed.fastq} --no-indels -e 2 --revcomp --cores=0 >> ../0-metadata/cutadapt_primer_trimming.txt\n",
    "\n",
    "done\n",
    "\n",
    "cd ../../\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "trimming primer seqs for: COI_100_HK37\n",
    "Done           00:00:00       146,040 reads @   3.4 µs/read;  17.58 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK38\n",
    "Done           00:00:00       118,917 reads @   3.7 µs/read;  16.14 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK39\n",
    "Done           00:00:00       162,326 reads @   3.5 µs/read;  17.39 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK40\n",
    "Done           00:00:00       115,722 reads @   3.8 µs/read;  15.94 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK41\n",
    "Done           00:00:00       102,320 reads @   3.8 µs/read;  15.74 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK42\n",
    "Done           00:00:00       148,463 reads @   3.6 µs/read;  16.78 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK49\n",
    "Done           00:00:00        91,171 reads @   4.1 µs/read;  14.62 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK50\n",
    "Done           00:00:00        80,114 reads @   4.3 µs/read;  14.11 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK51\n",
    "Done           00:00:00        83,923 reads @   4.7 µs/read;  12.89 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK52\n",
    "Done           00:00:00        74,255 reads @   4.2 µs/read;  14.24 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK53\n",
    "Done           00:00:00        76,890 reads @   4.3 µs/read;  14.04 M reads/minute\n",
    "trimming primer seqs for: COI_100_HK54\n",
    "Done           00:00:00        75,723 reads @   4.3 µs/read;  14.11 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK37\n",
    "Done           00:00:00       109,424 reads @   3.7 µs/read;  16.40 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK38\n",
    "Done           00:00:00        86,827 reads @   4.3 µs/read;  13.98 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK39\n",
    "Done           00:00:00       134,723 reads @   3.9 µs/read;  15.53 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK40\n",
    "Done           00:00:00       109,897 reads @   4.0 µs/read;  15.04 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK41\n",
    "Done           00:00:00       117,560 reads @   4.0 µs/read;  14.88 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK42\n",
    "Done           00:00:00        77,020 reads @   5.1 µs/read;  11.87 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK49\n",
    "Done           00:00:00        67,718 reads @   6.3 µs/read;   9.52 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK50\n",
    "Done           00:00:00        75,588 reads @   5.2 µs/read;  11.48 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK51\n",
    "Done           00:00:00       105,503 reads @   4.4 µs/read;  13.75 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK52\n",
    "Done           00:00:00       113,930 reads @   4.0 µs/read;  15.13 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK53\n",
    "Done           00:00:00       100,604 reads @   4.3 µs/read;  14.01 M reads/minute\n",
    "trimming primer seqs for: COI_500_HK54\n",
    "Done           00:00:00        95,343 reads @   4.3 µs/read;  13.86 M reads/minute\n",
    "trimming primer seqs for: NTC_1\n",
    "Done           00:00:00           306 reads @ 435.8 µs/read;   0.14 M reads/minute\n",
    "trimming primer seqs for: NTC_2\n",
    "Done           00:00:00         7,842 reads @  21.1 µs/read;   2.85 M reads/minute\n",
    "trimming primer seqs for: NTC_3\n",
    "Done           00:00:00         6,114 reads @  24.3 µs/read;   2.47 M reads/minute\n",
    "```\n",
    "````\n",
    "\n",
    "When executing this for loop, cutadapt will attempt to remove the primer sequences from all reads in both directions (parameter `--revcomp`). The output files containing the reads where the primers were successfully removed are placed in the subdirectory **sequenceData/4-demux/** and will have the extension **_trimmed.fastq** rather than **_merged.fastq**. As mentioned above, the cutadapt output was written to **sequenceData/0-metadata/cutadapt_primer_trimming.txt**, rather than the Terminal window. We can open this text file using a text editor, such as Sublime Text. However, due to the number of samples, it is difficult to get an overview of the success rate and compare the different samples, a similar issue we encountered when merging forward and reverse reads.\n",
    "\n",
    "```{figure} cutadaptstatistics2.png\n",
    ":name: Cutadapt statistics\n",
    "\n",
    ": Output of the cutadapt code, providing information about adapter removal success rate.\n",
    "```\n",
    "\n",
    "### 3.3 Summarizing cutadapt output\n",
    "\n",
    "To get a better overview of the read statistics after primer trimming and the success rate of cutadapt to locate and trim the primer-binding regions, we can alter the python code producing the bar graph to incorporate the sequence counts after primer trimming. This new version of the code takes in three arguments, the first is the location where the merged sequence files are stored, the second is the location of the raw sequence files, and the third is the location of the trimmed sequence files. Rather than trying to alter the code in the already-existing script **sequenceData/1-scripts/rawMergedStatistics.py**, we will generate a new script where we can simply cut and paste the code below. Generating, saving, and making the script executable follows the same process as discussed previously.\n",
    "\n",
    "```{code-block} bash\n",
    "nano sequenceData/1-scripts/rawMergedTrimmedStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "## import modules\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## user arguments\n",
    "mergedPath = sys.argv[1]\n",
    "rawPath = sys.argv[2]\n",
    "trimmedPath = sys.argv[3]\n",
    "\n",
    "## first, create a sample name list\n",
    "mergedFileList = os.listdir(mergedPath)\n",
    "sampleNameList = []\n",
    "for mergedFile in mergedFileList:\n",
    "    sampleName = mergedFile.split('_merged.fastq')[0]\n",
    "    if sampleName.startswith('COI_') or sampleName.startswith('NTC_'):\n",
    "        sampleNameList.append(sampleName)\n",
    "\n",
    "## count number of raw and merged sequences for each sample in sampleNameList\n",
    "rawSeqCount = {}\n",
    "mergedSeqCount = {}\n",
    "trimmedSeqCount = {}\n",
    "for sample in sampleNameList:\n",
    "    with open(f'{mergedPath}{sample}_merged.fastq', 'r') as mergedFile:\n",
    "        x = len(mergedFile.readlines()) / 4\n",
    "        mergedSeqCount[sample] = int(x)\n",
    "    with open(f'{rawPath}{sample}_1.fastq', 'r') as rawFile:\n",
    "        y = len(rawFile.readlines()) / 4\n",
    "        rawSeqCount[sample] = int(y)\n",
    "    with open(f'{trimmedPath}{sample}_trimmed.fastq', 'r') as trimmedFile:\n",
    "        z = len(trimmedFile.readlines()) / 4\n",
    "        trimmedSeqCount[sample] = int(z)\n",
    "\n",
    "## create a dataframe from the dictionaries\n",
    "df = pd.DataFrame({'Sample': list(rawSeqCount.keys()), 'Raw': list(rawSeqCount.values()), 'Merged': list(mergedSeqCount.values()), 'Trimmed': list(trimmedSeqCount.values())})\n",
    "\n",
    "## sort the dataframe by raw reads in descending order\n",
    "df = df.sort_values(by='Raw', ascending=False)\n",
    "\n",
    "## calculate the percentage of merged/raw and format it with 2 decimal places and the '%' symbol\n",
    "df['Percentage'] = (df['Trimmed'] / df['Raw'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "## create a horizontal bar plot using seaborn\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
    "\n",
    "## use seaborn's barplot\n",
    "ax = sns.barplot(x='Raw', y='Sample', data=df, label='Raw', color='#BBC6C8')\n",
    "sns.barplot(x='Merged', y='Sample', data=df, label='Merged', color='#469597')\n",
    "sns.barplot(x='Trimmed', y='Sample', data=df, label='Trimmed', color='#DDBEAA')\n",
    "\n",
    "## add labels and title\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Horizontal bar graph of raw, merged, and trimmed reads (Sorted by Total in Reverse)')\n",
    "\n",
    "## add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add raw read count next to the bars\n",
    "for i, v in enumerate(df['Percentage']):\n",
    "    ax.text(df['Raw'].values[i] + 50, i, v, va='center', fontsize=10, color='black')\n",
    "\n",
    "## save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('raw_and_merged_and_trimmed_bargraph.png', dpi = 300)\n",
    "```\n",
    "\n",
    "Press `ctrl +x` to exit out of the editor, followed by `y` and `return`.\n",
    "\n",
    "```{code-block} bash\n",
    "chmod +x sequenceData/1-scripts/rawMergedTrimmedStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "./sequenceData/1-scripts/rawMergedTrimmedStatistics.py sequenceData/2-raw/ sequenceData/2-raw/unzipped/ sequenceData/4-demux/\n",
    "```\n",
    "\n",
    "```{figure} raw_and_merged_and_trimmed_bargraph.png\n",
    ":name: Raw and merged and trimmed read count\n",
    "\n",
    ": Read count of raw, merged, and trimmed files.\n",
    "```\n",
    "\n",
    "The figure shows a similar success rate for cutadapt to locate and trim primers across all samples, with roughly 85% of raw sequences still included after merging and primer trimming. Interestingly, for the negative control samples (**NTC_1, NTC_2, and NTC_3**) hardly any reads are returned after primer trimming. To see what happened with the negative controls, let's run this little python code below to print out the cutadapt summary from the file we generated during the for loop. The python code takes two user arguments, which are (1) the file name of the cutadapt summary file to parse and (2) a list of sample names to print the summary statistics for. This list should be separated by `+` symbols.\n",
    "\n",
    "```{code-block} bash\n",
    "nano sequenceData/1-scripts/cutadaptParsing.py\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "## import modules\n",
    "import sys\n",
    "\n",
    "## user arguments\n",
    "cutadaptResults = sys.argv[1]\n",
    "sampleNames = sys.argv[2]\n",
    "\n",
    "## create sample list\n",
    "sampleList = sampleNames.split('+')\n",
    "\n",
    "## parse cutadapt file\n",
    "printLine = 0\n",
    "startPrint = 0\n",
    "with open(cutadaptResults, 'r') as infile:\n",
    "  for line in infile:\n",
    "    if line.startswith('=== Adapter 1 ==='):\n",
    "      startPrint = 0\n",
    "      printLine = 0\n",
    "    if startPrint == 1:\n",
    "      print(line.rstrip('\\n'))\n",
    "    if printLine == 1:\n",
    "      if line.startswith('=== Summary ==='):\n",
    "        startPrint = 1\n",
    "        print(line.rstrip('\\n'))\n",
    "    if line.startswith('Command line parameters:'):\n",
    "      for sample in sampleList:\n",
    "        if sample in line:\n",
    "          print(f'{sample} summary statistics:')\n",
    "          printLine = 1\n",
    "```\n",
    "\n",
    "Press `ctrl +x` to exit out of the editor, followed by `y` and `return`.\n",
    "\n",
    "```{code-block} bash\n",
    "chmod +x sequenceData/1-scripts/cutadaptParsing.py\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "./sequenceData/1-scripts/cutadaptParsing.py sequenceData/0-metadata/cutadapt_primer_trimming.txt NTC_1_merged.fastq+NTC_2_merged.fastq+NTC_3_merged.fastq\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "NTC_1_merged.fastq summary statistics:\n",
    "=== Summary ===\n",
    "\n",
    "Total reads processed:                     306\n",
    "Reads with adapters:                       306 (100.0%)\n",
    "Reverse-complemented:                        0 (0.0%)\n",
    "\n",
    "== Read fate breakdown ==\n",
    "Reads that were too short:                  77 (25.2%)\n",
    "Reads that were too long:                    3 (1.0%)\n",
    "Reads discarded as untrimmed:                0 (0.0%)\n",
    "Reads written (passing filters):           226 (73.9%)\n",
    "\n",
    "Total basepairs processed:        88,071 bp\n",
    "Total written (filtered):         70,668 bp (80.2%)\n",
    "\n",
    "NTC_2_merged.fastq summary statistics:\n",
    "=== Summary ===\n",
    "\n",
    "Total reads processed:                   7,842\n",
    "Reads with adapters:                     7,839 (100.0%)\n",
    "Reverse-complemented:                        0 (0.0%)\n",
    "\n",
    "== Read fate breakdown ==\n",
    "Reads that were too short:               7,691 (98.1%)\n",
    "Reads that were too long:                    6 (0.1%)\n",
    "Reads discarded as untrimmed:                0 (0.0%)\n",
    "Reads written (passing filters):           145 (1.8%)\n",
    "\n",
    "Total basepairs processed:       507,546 bp\n",
    "Total written (filtered):         45,343 bp (8.9%)\n",
    "\n",
    "NTC_3_merged.fastq summary statistics:\n",
    "=== Summary ===\n",
    "\n",
    "Total reads processed:                   6,114\n",
    "Reads with adapters:                     6,105 (99.9%)\n",
    "Reverse-complemented:                        0 (0.0%)\n",
    "\n",
    "== Read fate breakdown ==\n",
    "Reads that were too short:               5,928 (97.0%)\n",
    "Reads that were too long:                    7 (0.1%)\n",
    "Reads discarded as untrimmed:                0 (0.0%)\n",
    "Reads written (passing filters):           179 (2.9%)\n",
    "\n",
    "Total basepairs processed:       440,273 bp\n",
    "Total written (filtered):         55,985 bp (12.7%)\n",
    "```\n",
    "````\n",
    "\n",
    "The cutadapt summary statistics for the negative control samples shows that a large proportion of the reads were discarded as they were too short (NTC_2: 98.1%; NTC_3: 97.0%). This probably meant that some primer-dimer sequences seeped through during the size selection step of the library preparation protocol and got sequenced. While primer-dimers should be removed during library preparation, as Illumina favours sequencing shorter fragments, such a low number as observed in this tutorial data set is a normal occurrence and nothing to be worried about.\n",
    "\n",
    "## 4. Quality filtering\n",
    "\n",
    "During the bioinformatic pipeline, it is critical to only retain high-quality reads to reduce the abundance and impact of spurious sequences. There is an intrinsic error rate to all polymerases used during PCR amplification, as well as sequencing technologies. For example, the most frequently used polymerase during PCR is *Taq*, though lacks 3' to 5' exonuclease proofreading activity, resulting in relatively low replication fidelity. These errors will generate some portion of sequences that vary from their biological origin sequence. Such reads can substantially inflate metrics such as alpha diversity, especially in a denoising approach (more on this later). While it is near-impossible to remove all of these sequences bioinformatically, especially PCR errors, we will attempt to remove erroneous reads by filtering on base calling quality scores (the fourth line of a sequence record in a .fastq file).\n",
    "\n",
    "```{important}\n",
    "When processing your own metabarcoding data, I suggest you follow the same structure as we did with merging and trimming reads, i.e., check the code on one sample, play around with the parameter settings, and only batch process your samples when everything works fine. However, to save time during this tutorial, we will move straight to batch processing the samples for the quality filtering steps with the correct parameters.\n",
    "```\n",
    "\n",
    "### 4.1 Batch processing\n",
    "\n",
    "For quality filtering, we will be using the `--fastq_filter` command in [VSEARCH](https://github.com/torognes/vsearch). During this step, we will discard all sequences that do not adhere to a specific set of rules. Quality filtering parameters are not standardized, but rather specific for each library. For our tutorial data, we will filter out all sequences on a much stricter size range compared to the settings we used during primer trimming (`--fastq_minlen 310` and `--fastq_maxlen 316`). Additionally, we will remove sequences that have ambiguous base calls, bases denotes as N, rather than A, C, G, or T (`--fastq_maxns 0`). Ambiguous base calls, however, are not a real issue with Illumina sequencing data. The last parameter we will use to filter our sequences is a maximum expected error rate (`--fastq_maxee 1.0`). As this is the last step in our bioinformatic pipeline where quality scores are essential, we will export our output files both in .fastq and .fasta format. Additionally, we will be merging all files together after this step in the bioinformatic pipeline. Before merging all files, however, we need to change the sequence headers to contain the information to which sample the sequence belongs to, which is currently stored in the file name. We can rename sequence header based on the file name using the `--relabel` parameter.\n",
    "\n",
    "```{code-block} bash\n",
    "cd sequenceData/4-demux/\n",
    "for fq in *_trimmed.fastq\n",
    "do\n",
    "\n",
    "  echo \"Merging reads for: ${fq/_trimmed.fastq/}\"\n",
    "  vsearch --fastq_filter ${fq} --fastq_maxee 1.0 --fastq_maxlen 316 --fastq_minlen 310 --fastq_maxns 0 --fastqout ../5-filter/${fq/_trimmed.fastq/_filtered.fastq} --fastaout ../5-filter/${fq/_trimmed.fastq/_filtered.fasta} --relabel ${fq/_trimmed.fastq/}.\n",
    "\n",
    "done\n",
    "\n",
    "cd ../../\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "Merging reads for: COI_100_HK37\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "128093 sequences kept (of which 0 truncated), 8316 sequences discarded.\n",
    "Merging reads for: COI_100_HK38\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "105809 sequences kept (of which 0 truncated), 6969 sequences discarded.\n",
    "Merging reads for: COI_100_HK39\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "145327 sequences kept (of which 0 truncated), 9152 sequences discarded.\n",
    "Merging reads for: COI_100_HK40\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "103665 sequences kept (of which 0 truncated), 6297 sequences discarded.\n",
    "Merging reads for: COI_100_HK41\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "91076 sequences kept (of which 0 truncated), 5477 sequences discarded.\n",
    "Merging reads for: COI_100_HK42\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "134371 sequences kept (of which 0 truncated), 8278 sequences discarded.\n",
    "Merging reads for: COI_100_HK49\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "82338 sequences kept (of which 0 truncated), 5241 sequences discarded.\n",
    "Merging reads for: COI_100_HK50\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "72895 sequences kept (of which 0 truncated), 4298 sequences discarded.\n",
    "Merging reads for: COI_100_HK51\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "75547 sequences kept (of which 0 truncated), 5111 sequences discarded.\n",
    "Merging reads for: COI_100_HK52\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "67509 sequences kept (of which 0 truncated), 3961 sequences discarded.\n",
    "Merging reads for: COI_100_HK53\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "70090 sequences kept (of which 0 truncated), 4008 sequences discarded.\n",
    "Merging reads for: COI_100_HK54\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "68948 sequences kept (of which 0 truncated), 3952 sequences discarded.\n",
    "Merging reads for: COI_500_HK37\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "98678 sequences kept (of which 0 truncated), 6897 sequences discarded.\n",
    "Merging reads for: COI_500_HK38\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "78856 sequences kept (of which 0 truncated), 4859 sequences discarded.\n",
    "Merging reads for: COI_500_HK39\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "122200 sequences kept (of which 0 truncated), 7674 sequences discarded.\n",
    "Merging reads for: COI_500_HK40\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "99565 sequences kept (of which 0 truncated), 6561 sequences discarded.\n",
    "Merging reads for: COI_500_HK41\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "106658 sequences kept (of which 0 truncated), 6567 sequences discarded.\n",
    "Merging reads for: COI_500_HK42\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "70167 sequences kept (of which 0 truncated), 4090 sequences discarded.\n",
    "Merging reads for: COI_500_HK49\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "61051 sequences kept (of which 0 truncated), 4086 sequences discarded.\n",
    "Merging reads for: COI_500_HK50\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "68680 sequences kept (of which 0 truncated), 4195 sequences discarded.\n",
    "Merging reads for: COI_500_HK51\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "95654 sequences kept (of which 0 truncated), 6049 sequences discarded.\n",
    "Merging reads for: COI_500_HK52\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "104924 sequences kept (of which 0 truncated), 4927 sequences discarded.\n",
    "Merging reads for: COI_500_HK53\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "91320 sequences kept (of which 0 truncated), 5605 sequences discarded.\n",
    "Merging reads for: COI_500_HK54\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "87233 sequences kept (of which 0 truncated), 4500 sequences discarded.\n",
    "Merging reads for: NTC_1\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "161 sequences kept (of which 0 truncated), 65 sequences discarded.\n",
    "Merging reads for: NTC_2\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "86 sequences kept (of which 0 truncated), 59 sequences discarded.\n",
    "Merging reads for: NTC_3\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "92 sequences kept (of which 0 truncated), 87 sequences discarded.\n",
    "```\n",
    "````\n",
    "\n",
    "```````{admonition} Exercise 3\n",
    ":class: hint\n",
    "\n",
    "How would you check if the sequence headers of the .fastq and .fasta output files are relabeled and contain the sample information?\n",
    "\n",
    "``````{admonition} Answer 3\n",
    ":class: title, dropdown\n",
    "`````{tab-set}\n",
    "````{tab-item} bash: head\n",
    "Using the `head` command, we can print the first N lines of a document.\n",
    "```{code-block} bash\n",
    "head -n 1 sequenceData/4-demux/COI_100_HK37_trimmed.fastq\n",
    "head -n 1 sequenceData/5-filter/COI_100_HK37_filtered.fastq\n",
    "head -n 1 sequenceData/5-filter/COI_100_HK37_filtered.fasta\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab-item} bash: tail\n",
    "Using the `tail` command we can print the last N lines of a document. Note the different number of lines needed to be printed for the .fasta file.\n",
    "```{code-block} bash\n",
    "tail -n 4 sequenceData/4-demux/COI_100_HK37_trimmed.fastq\n",
    "tail -n 4 sequenceData/5-filter/COI_100_HK37_filtered.fastq\n",
    "tail -n 5 sequenceData/5-filter/COI_100_HK37_filtered.fasta\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab-item} bash: grep\n",
    "Using the `grep` command on the sample name within the file, we can verify its presence in the file and also count the number of lines (sequences) that contain the sample information. Note the different search term for the .fasta file, due to the different sequence header structure. The `grep` output for the filtered .fastq and .fasta files should be identical.\n",
    "```{code-block} bash\n",
    "grep -c \"^@COI_100_HK37\" sequenceData/4-demux/COI_100_HK37_trimmed.fastq\n",
    "grep -c \"^@COI_100_HK37\" sequenceData/5-filter/COI_100_HK37_filtered.fastq\n",
    "grep -c \"^>COI_100_HK37\" sequenceData/5-filter/COI_100_HK37_filtered.fasta\n",
    "```\n",
    "````\n",
    "`````\n",
    "``````\n",
    "```````\n",
    "\n",
    "### 4.2 Summary output\n",
    "\n",
    "While VSEARCH provides the numbers for each sample before and after quality filtering, we will update the python script that generates the bar graph again, now to also include the filtered reads. This new python script takes in four user arguments, the same three as before, plus the path to where the filtered sequence files are stored. This will allow us to easily identify how the quality filtering performed and compare differences between samples.\n",
    "\n",
    "```{code-block} bash\n",
    "nano sequenceData/1-scripts/rawMergedTrimmedFilteredStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "## import modules\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## user arguments\n",
    "mergedPath = sys.argv[1]\n",
    "rawPath = sys.argv[2]\n",
    "trimmedPath = sys.argv[3]\n",
    "filteredPath = sys.argv[4]\n",
    "\n",
    "\n",
    "## first, create a sample name list\n",
    "mergedFileList = os.listdir(mergedPath)\n",
    "sampleNameList = []\n",
    "for mergedFile in mergedFileList:\n",
    "    sampleName = mergedFile.split('_merged.fastq')[0]\n",
    "    if sampleName.startswith('COI_') or sampleName.startswith('NTC_'):\n",
    "        sampleNameList.append(sampleName)\n",
    "\n",
    "## count number of raw and merged sequences for each sample in sampleNameList\n",
    "rawSeqCount = {}\n",
    "mergedSeqCount = {}\n",
    "trimmedSeqCount = {}\n",
    "filteredSeqCount = {}\n",
    "for sample in sampleNameList:\n",
    "    with open(f'{mergedPath}{sample}_merged.fastq', 'r') as mergedFile:\n",
    "        x = len(mergedFile.readlines()) / 4\n",
    "        mergedSeqCount[sample] = int(x)\n",
    "    with open(f'{rawPath}{sample}_1.fastq', 'r') as rawFile:\n",
    "        y = len(rawFile.readlines()) / 4\n",
    "        rawSeqCount[sample] = int(y)\n",
    "    with open(f'{trimmedPath}{sample}_trimmed.fastq', 'r') as trimmedFile:\n",
    "        z = len(trimmedFile.readlines()) / 4\n",
    "        trimmedSeqCount[sample] = int(z)\n",
    "    with open(f'{filteredPath}{sample}_filtered.fastq', 'r') as filteredFile:\n",
    "        a = len(filteredFile.readlines()) / 4\n",
    "        filteredSeqCount[sample] = int(a)\n",
    "\n",
    "\n",
    "## create a dataframe from the dictionaries\n",
    "df = pd.DataFrame({'Sample': list(rawSeqCount.keys()), 'Raw': list(rawSeqCount.values()), 'Merged': list(mergedSeqCount.values()), 'Trimmed': list(trimmedSeqCount.values()), 'Filtered': list(filteredSeqCount.values())})\n",
    "\n",
    "## sort the dataframe by raw reads in descending order\n",
    "df = df.sort_values(by='Raw', ascending=False)\n",
    "\n",
    "## calculate the percentage of merged/raw and format it with 2 decimal places and the '%' symbol\n",
    "df['Percentage'] = (df['Filtered'] / df['Raw'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "\n",
    "## create a horizontal bar plot using seaborn\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
    "\n",
    "## use seaborn's barplot\n",
    "ax = sns.barplot(x='Raw', y='Sample', data=df, label='Raw', color='#BBC6C8')\n",
    "sns.barplot(x='Merged', y='Sample', data=df, label='Merged', color='#469597')\n",
    "sns.barplot(x='Trimmed', y='Sample', data=df, label='Trimmed', color='#DDBEAA')\n",
    "sns.barplot(x='Filtered', y='Sample', data=df, label='Filtered', color='#806491')\n",
    "\n",
    "## add labels and title\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Horizontal bar graph of raw, merged, trimmed, and filtered reads (Sorted by Total in Reverse)')\n",
    "\n",
    "## add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add raw read count next to the bars\n",
    "for i, v in enumerate(df['Percentage']):\n",
    "    ax.text(df['Raw'].values[i] + 50, i, v, va='center', fontsize=10, color='black')\n",
    "\n",
    "## save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('raw_and_merged_and_trimmed_and_filtered_bargraph.png', dpi = 300)\n",
    "```\n",
    "\n",
    "Press `ctrl +x` to exit out of the editor, followed by `y` and `return`.\n",
    "\n",
    "```{code-block} bash\n",
    "chmod +x sequenceData/1-scripts/rawMergedTrimmedFilteredStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "./sequenceData/1-scripts/rawMergedTrimmedFilteredStatistics.py sequenceData/2-raw/ sequenceData/2-raw/unzipped/ sequenceData/4-demux/ sequenceData/5-filter/\n",
    "```\n",
    "\n",
    "```{figure} raw_and_merged_and_trimmed_and_filtered_bargraph.png\n",
    ":name: Raw and merged and trimmed and filtered read count\n",
    "\n",
    ": Read count of raw, merged, trimmed, and filtered files.\n",
    "```\n",
    "\n",
    "This bar graph shows that after quality filtering, we still keep around 80% of the raw reads, which is very good and due to the high quality data we started with (remember the multiQC output from before!). Again, we see a big difference between the actual samples and negative controls is observed.\n",
    "\n",
    "### 4.3 Check filtering step\n",
    "\n",
    "Before continuing with the bioinformatic pipeline, it is best to check if the quality filtering step retained only high-quality sequences that are of length ~313 bp (estimated length of the amplicon). If quality filtering was successful, we can move on to the next steps in our bioinformatic pipeline. The check the quality and length of our filtered sequence files, we will use [FASTQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [MULTIQC](https://multiqc.info), same as before.\n",
    "\n",
    "```{code-block} bash\n",
    "fastqc sequenceData/5-filter/*.fastq -o sequenceData/3-fastqc/ -t 8\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "Started analysis of COI_100_HK37_filtered.fastq\n",
    "Approx 5% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 10% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 15% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 20% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 25% complete for COI_100_HK37_filtered.fastq\n",
    "Started analysis of COI_100_HK38_filtered.fastq\n",
    "Approx 30% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 5% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 35% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 10% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 40% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 15% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 45% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 20% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 50% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 25% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 30% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 55% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 35% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 60% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 40% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 65% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 45% complete for COI_100_HK38_filtered.fastq\n",
    "Approx 70% complete for COI_100_HK37_filtered.fastq\n",
    "Approx 50% complete for COI_100_HK38_filtered.fastq\n",
    "...\n",
    "```\n",
    "````\n",
    "\n",
    "FastQC will generate a .html report for every single .fastq file in the subdirectory **sequenceData/5-filter/**. Since we are working with 27 files, it will be easier to compare the reports by collating them, something we can do using multiQC.\n",
    "\n",
    "```{code-block} bash\n",
    "multiqc sequenceData/3-fastqc/*filtered* -o sequenceData/3-fastqc/\n",
    "```\n",
    "\n",
    "The multiQC program will combine all 27 FastQC reports into a single .html document. Let's open this to see how our filtered sequence data is looking like. Since we have written both multiQC reports (raw and filtered) to the same **sequenceData/3-fastqc/** directory, make sure to open the latest report. The file name will most likely end in **_1.html**.\n",
    "\n",
    "```{figure} multiqcreportfiltered2.png\n",
    ":name: multiQC report filtered\n",
    "\n",
    ": The .html multiQC report for the quality filtered reads\n",
    "```\n",
    "\n",
    "From this multiQC report, we can see that we only retained high quality reads that are of length 313 bp. Exactly what we need to move forward with our bioinformatic pipeline!\n",
    "\n",
    "## 5. Dereplication\n",
    "\n",
    "### 5.1 Combining data into one file\n",
    "\n",
    "Once we have verified that the reads passing quality filtering are of high quality and of a length similar to the expected amplicon size, we can move forward with our bioinformatic pipeline. For the next final steps of the bioinformatic pipeline, it makes more sense to combine all the files into a single file. So, let's do this first using the `cat` command, which stands for *concatenate*. Since base calling quality scores are not essential for our analysis from this point onwards, we can combine all the .fasta files to reduce the file size and computational cost. To concatenate all files, we can use the `*` symbol. Make sure to specify the output file using `>`, otherwise `cat` will print the combined file to the Terminal window instead.\n",
    "\n",
    "```{code-block} bash\n",
    "cat sequenceData/5-filter/*.fasta > sequenceData/6-quality/combined.fasta\n",
    "```\n",
    "\n",
    "```````{admonition} Exercise 4\n",
    ":class: hint\n",
    "\n",
    "How would you determine if no sequences were lost during the `cat` command to merge all .fasta files?\n",
    "\n",
    "``````{admonition} Answer 4\n",
    ":class: title, dropdown\n",
    "\n",
    "First, let's start with the easiest problem, i.e., determine the number of sequences in the merged file. Here we can simply pipe the output of `wc -l` to the `awk` command and divide the number of lines by 5 (= one sequence record).\n",
    "\n",
    "```{code-block} bash\n",
    "wc -l sequenceData/6-quality/combined.fasta | awk '{print $1/5}'\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note\n",
    "```\n",
    "2230993\n",
    "```\n",
    "````\n",
    "\n",
    "To count the total number of sequences in the separate files before concatenation, we will need to write a for loop to count all the lines in each file, pipe that number to `awk` to divide it by 5, and sum the number of sequences for each file.\n",
    "\n",
    "```{code-block} bash\n",
    "total_lines=0\n",
    "for item in sequenceData/5-filter/*.fasta \n",
    "do\n",
    "    lines=$(wc -l \"$item\" | awk '{print $1/5}')\n",
    "    total_lines=$((total_lines + lines))\n",
    "done\n",
    "\n",
    "echo \"$total_lines\"\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note\n",
    "```\n",
    "2230993\n",
    "```\n",
    "````\n",
    "``````\n",
    "```````\n",
    "\n",
    "### 5.2 Finding unique sequences\n",
    "\n",
    "With the `cat` command, we have created a single file. The next step in our bioinformatic pipeline is to dereplicate our data, i.e., find unique sequences. Since metabarcoding data is based on a PCR amplification method, the same DNA molecule will have been copied thousands or millions of times and, therefore, sequenced multiple times. In order to reduce the file size and computational cost, it is convenient to work with unique sequences and keep track of how many times each unique sequence was found in our data set. This dereplication step is also the reason why we combined all samples into a single file, as the same barcode (or species if you will) can be found across different samples.\n",
    "\n",
    "We can dereplicate our data by using the `--derep_fulllength` command in [VSEARCH](https://github.com/torognes/vsearch). The `--sizeout` parameter keeps a tally of the number of times each unique sequence was observed across all samples and places this information in the sequence header. Since we are looking at unique sequences across samples, it wouldn't make much sense to keep the current sequence header information, as it currently refers to which sample a sequence belongs to. We will, therefore, rename the sequence headers by using the `--relabel` parameter.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --derep_fulllength sequenceData/6-quality/combined.fasta --sizeout --relabel uniq. --output sequenceData/6-quality/uniques.fasta\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Dereplicating file sequenceData/6-quality/combined.fasta 100%  \n",
    "697630765 nt in 2230993 seqs, min 310, max 316, avg 313\n",
    "Sorting 100%\n",
    "244379 unique sequences, avg cluster 9.1, median 1, max 68022\n",
    "Writing FASTA output file 100% \n",
    "```\n",
    "````\n",
    "\n",
    "We can see from the VSEARCH output that 244,379 out of 2,230,993 sequences were unique. The average number a unique sequence was observed was 9.1 times, while the median is 1 and the maximum is 68,022 times. As mentioned before, the `--sizeout` parameter will have added the abundance information for each unique sequence to the header. Let's use the `head` command to investigate the first couple of sequences.\n",
    "\n",
    "```{code-block} bash\n",
    "head -n 10 sequenceData/6-quality/uniques.fasta\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    ">uniq.1;size=68022\n",
    "CCTAAGCTCCAATATTGCCCACGCCGGGGCGTCTGTTGACCTTGCTATCTTTAGGCTACACTTGGCTGGGGTTTCTTCTC\n",
    "TACTCGGGGCTGTAAACTTTATTAGAACTATCGCTAACCTGCGAGCTTTAGGGCTAATTCTTGACCGTATAACACTATTC\n",
    "ACATGATCAGTTCTTATCACCGCCATCCTTCTCCTTCTTTCTCTACCTGTTCTCGCAGGGGCTATTACGATACTCCTTAC\n",
    "CGACCGAAATCTAAATACCTCTTTTTATGACCCTAGAGGAGGGGGAGACCCTATTCTCTACCAACACCTGTTT\n",
    ">uniq.2;size=59045\n",
    "CCTCTCCGCTAGCGGAGCCCATGGATCTGCCTCAGTAGATTTAAGGATCTTTTCCCTCCATCTAGCAGGAGTATCTTCTA\n",
    "TCCTCGGATCCATCAACTTTATCACCACAATTATTAATATGCGAACCCCAAACCTCACATGAGACAAAATCTCTCTCTTT\n",
    "ACTTGATCCATCCTTATTACTACTATCCTACTCCTCCTCTCTCTCCCTGTCCTAGCCGGAGCTTTAACTATACTTCTAAC\n",
    "AGACCGTAACTTCAACACAACATTTTTTGACCCTAGAGGAGGAGGCGACCCTATCCTTTACCAGCACCTTTTC\n",
    "```\n",
    "````\n",
    "\n",
    "The output of the `head` command shows that the unique sequences are already sorted by abundance, with the most abundant unique sequence occurring 68,022 times and the second most abundant sequence occurring 59,045 times.\n",
    "\n",
    "````````{admonition} Exercise 5\n",
    ":class: hint\n",
    "\n",
    "How would you determine the number of times the 10 most and 10 least abundant unique sequences were observed?\n",
    "\n",
    "```````{admonition} Answer 5\n",
    ":class: title, dropdown\n",
    "``````{tab-set}\n",
    "`````{tab-item} Two-liner\n",
    "The simplest way to determine the abundance of the 10 most and 10 least abundant unique sequences, based on the code we've learned during this tutorial thus far, would be to write two lines of code, one for the 10 most abundant and one for the 10 least abundant. We can solve this problem by taking out all the lines that start with `^>uniq` using `grep` and pipe that list to either the `head -n 10` or `tail -n 10` command.\n",
    "```{code-block} bash\n",
    "grep '^>uniq' sequenceData/6-quality/uniques.fasta | head -n 10\n",
    "grep '^>uniq' sequenceData/6-quality/uniques.fasta | tail -n 10\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note\n",
    "```\n",
    ">uniq.1;size=68022\n",
    ">uniq.2;size=59045\n",
    ">uniq.3;size=57634\n",
    ">uniq.4;size=42670\n",
    ">uniq.5;size=42507\n",
    ">uniq.6;size=40947\n",
    ">uniq.7;size=38849\n",
    ">uniq.8;size=38764\n",
    ">uniq.9;size=37125\n",
    ">uniq.10;size=34995\n",
    "\n",
    ">uniq.244370;size=1\n",
    ">uniq.244371;size=1\n",
    ">uniq.244372;size=1\n",
    ">uniq.244373;size=1\n",
    ">uniq.244374;size=1\n",
    ">uniq.244375;size=1\n",
    ">uniq.244376;size=1\n",
    ">uniq.244377;size=1\n",
    ">uniq.244378;size=1\n",
    ">uniq.244379;size=1\n",
    "```\n",
    "````\n",
    "`````\n",
    "\n",
    "`````{tab-item} One-liner\n",
    "To solve this problem with one line of code, we need to use `awk`. As mentioned before, the syntax of this language is quite complex. To provide a bit of background to the solution: \n",
    "1. `/^>uniq/`: is the pattern we are looking for (similar to `grep`). \n",
    "2. `{a[i++]=$0}`: when a line matches the pattern, it is added to an array `a`, and the index `i` is incremented. `$0` represents the entire line.\n",
    "3. `END`: the next section of the code is only executed after processing all lines in the input file.\n",
    "4. `for (j=i-10; j<i; j++) print a[j]`: this loop prints the last 10 lines that matched the pattern. It starts from `i - 10` to `i`, where `i` is the number of lines that matched the pattern, and prints those lines from the array `a`.\n",
    "5. `for (k=0; k<10; k++) print a[k]`: this loop prints the first 10 lines that matched the pattern, iterating from `0` to `9` (first 10 elements) and prints those lines from the array `a`.\n",
    "```{code-block} bash\n",
    "awk '/^>uniq/ {a[i++]=$0} END {for (j=i-10; j<i; j++) print a[j]; for (k=0; k<10; k++) print a[k]}' sequenceData/6-quality/uniques.fasta\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note\n",
    "```\n",
    ">uniq.244370;size=1\n",
    ">uniq.244371;size=1\n",
    ">uniq.244372;size=1\n",
    ">uniq.244373;size=1\n",
    ">uniq.244374;size=1\n",
    ">uniq.244375;size=1\n",
    ">uniq.244376;size=1\n",
    ">uniq.244377;size=1\n",
    ">uniq.244378;size=1\n",
    ">uniq.244379;size=1\n",
    ">uniq.1;size=68022\n",
    ">uniq.2;size=59045\n",
    ">uniq.3;size=57634\n",
    ">uniq.4;size=42670\n",
    ">uniq.5;size=42507\n",
    ">uniq.6;size=40947\n",
    ">uniq.7;size=38849\n",
    ">uniq.8;size=38764\n",
    ">uniq.9;size=37125\n",
    ">uniq.10;size=34995\n",
    "```\n",
    "`````\n",
    "``````\n",
    "```````\n",
    "````````\n",
    "\n",
    "## 6. Denoising\n",
    "\n",
    "Now that our data set is filtered and unique sequences have been retrieved, we are ready for the next step in the bioinformatic pipeline, i.e., looking for **biologically meaningful** or **biologically correct** sequences. Two approaches to achieve this goal exist, including **denoising** and **clustering**. There is still an ongoing debate on what the best approach is to obtain these **biologicall meaningful** sequences. For more information, these are two good papers to start with: [Brandt et al., 2021](https://onlinelibrary.wiley.com/doi/full/10.1111/1755-0998.13398) and [Antich et al., 2021](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04115-6). For this tutorial we won't be discussing this topic in much detail, but this is the basic idea...\n",
    "\n",
    "When **clustering** the dataset, OTUs (Operational Taxonomic Units) will be generated by combining sequences that are similar to a set percentage level (traditionally 97%), with the most abundant sequence identified as the true sequence. When clustering at 97%, this means that if a sequence is more than 3% different than the generated OTU, a second OTU will be generated. The concept of OTU clustering was introduced in the 1960s and has been debated since. Clustering the dataset is usually used for identifying species in metabarcoding data.\n",
    "\n",
    "**Denoising**, on the other hand, attempts to identify all correct biological sequences through an algorithm. In short, denoising will cluster the data with a 100% threshold and tries to identify errors based on abundance differences. The retained sequences are called ZOTU (Zero-radius Operation Taxonomic Unit) or ASVs (Amplicon Sequence Variants). Denoising the dataset is usually used for identifying intraspecific variation in metabarcoding data. A schematic of both approaches can be found below.\n",
    "\n",
    "```{figure} denoisingandclustering.png\n",
    ":name: Schematic of denoising and clustering\n",
    "\n",
    ": A schematic representation of denoising and clustering. Copyright: [Antich et al., 2021](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04115-6)\n",
    "```\n",
    "\n",
    "This difference in approach may seem small but has a very big impact on your final dataset!\n",
    "\n",
    "When you denoise the dataset, it is expected that one species may have more than one ZOTU. This means that when the reference database is incomplete, or you plan to work with ZOTUs instead of taxonomic assignments, your diversity estimates will be highly inflated. When clustering the dataset, on the other hand, it is expected that an OTU may have more than one species assigned to it, meaning that you may lose some correct biological sequences that are present in your data by merging species with barcodes more similar than 97%. In other words, you will miss out on differentiating closely related species and intraspecific variation.\n",
    "\n",
    "For this tutorial, we will use the **denoising** approach, as it is favoured in recent years. However, clustering is still a valid option. So, feel free to explore this approach for your own data set if you prefer.\n",
    "\n",
    "For denoising, we will use the [unoise3 algorithm](https://drive5.com/usearch/manual/cmd_unoise3.html) as implemented in the `--cluster_unoise` command in [VSEARCH](https://github.com/torognes/vsearch). Since denoising is based on read abundance of the unique sequences, we can specify the `--sizein` parameter. The minimum abundance threshold for a true denoised read is defaulted to 8 reads, as specified by the unoise3 algorithm developer. However, more recent research by [Bokulich et al., 2013](https://www.nature.com/articles/nmeth.2276), identified a minimum abundance threshold to be more appropriate. Hence, we will set the `--minsize` parameter to 0.001%, which in our case is ~22 reads. As we will be merging different reads with varying abundances, we need to recalculate the new count for each denoised read using the `--sizeout` parameter. Relabeling the sequence headers can be done through the `--relabel` parameter and the output file is specified using `--centroids`.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --cluster_unoise sequenceData/6-quality/uniques.fasta --sizein --minsize 22 --sizeout --relabel denoised. --centroids sequenceData/6-quality/denoised.fasta\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading file sequenceData/6-quality/uniques.fasta 100%  \n",
    "1519235 nt in 4859 seqs, min 310, max 316, avg 313\n",
    "minsize 22: 239520 sequences discarded.\n",
    "Masking 100% \n",
    "Sorting by abundance 100%\n",
    "Counting k-mers 100% \n",
    "Clustering 100%  \n",
    "Sorting clusters 100%\n",
    "Writing clusters 100% \n",
    "Clusters: 1372 Size min 22, max 187320, avg 3.5\n",
    "Singletons: 0, 0.0% of seqs, 0.0% of clusters\n",
    "```\n",
    "````\n",
    "\n",
    "From the output, we can see that we have generated 1,372 denoised reads and discarded 239,520 unique sequences, as they did not achieve an abundance of at least 22 reads.\n",
    "\n",
    "## 7. Chimera removal\n",
    "\n",
    "The second to last step in our bioinformatic pipeline is to remove chimeric sequences. Amplicon sequencing has the potential to generate chimeric reads, which can cause spurious inference of biological variation. Chimeric amplicons form when an incomplete DNA strand anneals to a different template and primes synthesis of a new template derived from two different biological sequences, or in other words chimeras are artefact sequences formed by two or more biological sequences incorrectly joined together. More information can be found on this [website](https://www.biorxiv.org/content/biorxiv/early/2016/09/09/074252.full.pdf) and a simple illustration can be found below.\n",
    "\n",
    "```{figure} chimeras.jpg\n",
    ":name: Schematic of chimera formation\n",
    "\n",
    ": A schematic representation of chimera formation. Copyright: [Genome Research](https://genome.cshlp.org/content/21/3/494/F1.expansion.html)\n",
    "```\n",
    "\n",
    "We will use the `--uchime3_denovo` algorithm as implemented in [VSEARCH](https://github.com/torognes/vsearch) for removing chimeric sequences from our denoised reads. This method is also based on sequence abundance, hence, we need to provide the `--sizein` parameter. As the output (parameter `--nonchimeras`) file will contain our **biologically relevant** sequences, we will relabel our sequence headers using `--relabel zotu.`.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --uchime3_denovo sequenceData/6-quality/denoised.fasta --sizein --nonchimeras sequenceData/8-final/zotus.fasta --relabel zotu.\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading file sequenceData/6-quality/denoised.fasta 100%  \n",
    "429255 nt in 1372 seqs, min 310, max 316, avg 313\n",
    "Masking 100% \n",
    "Sorting by abundance 100%\n",
    "Counting k-mers 100% \n",
    "Detecting chimeras 100%  \n",
    "Found 27 (2.0%) chimeras, 1345 (98.0%) non-chimeras,\n",
    "and 0 (0.0%) borderline sequences in 1372 unique sequences.\n",
    "Taking abundance information into account, this corresponds to\n",
    "1085 (0.1%) chimeras, 1785492 (99.9%) non-chimeras,\n",
    "and 0 (0.0%) borderline sequences in 1786577 total sequences.\n",
    "```\n",
    "````\n",
    "\n",
    "The output shows that 98% of our denoised reads were kept and 27 sequences were identified as chimeric. Taking into account abundance information, this would result into 0.1% of reads identified as chimeras.\n",
    "\n",
    "**The *sequenceData/8-final/zotus.fasta* file is the first output file we have created that we need for our statistical analysis!**\n",
    "\n",
    "```{important}\n",
    "If you use the unoise3 algorithm as implemented in USEARCH, chimera removal is built into the denoising step and will not have to be conducted separately!\n",
    "```\n",
    "\n",
    "## 8. Frequency table\n",
    "\n",
    "Now that we have created our list of **biologically relevant** sequences or ZOTUs or \"species\", we are ready to generate a frequency table, also known as a count table. This will be the last step in our bioinformatic pipeline. A frequency table is something you might have encountered before during some traditional ecological surveys you have conducted, whereby a table was created with site names as column headers, a species list as rows, and the number in each cell representing the number of individuals observed for a specific species at a specific site.\n",
    "\n",
    "In metabarcoding studies, a frequency table is analogous, where it tells us how many times each sequence has appeared in each sample. It is the end-product of all the bioinformatic processing steps we have conducted today. Now that we have identified what we believe to be true biological sequences, we are going to generate our frequency table by matching the merged sequences to our ZOTU sequence list. Remember that the sequences within the **sequenceData/6-quality/combined.fasta** file have the information of the sample they belong to present in the sequence header, which is how the `--usearch_global` command in [VSEARCH](https://github.com/torognes/vsearch) can generate the frequency table. The `--db` parameter allows us to set the ZOTU sequence list (**sequenceData/8-final/zotus.fasta**) as the database to search against, while we can specify the `--strand` parameter as *plus*, since all sequences are in the same direction after primer trimming. Finally, we need to incorporate a 97% identity threshold for this function through the `--id` parameter. This might seem counter-intuitive, since we employed a denoising approach. However, providing some leniency on which sequences can map to our ZOTU will allow us to incorporate a larger percentage of the data set. As some ZOTUs might be more similar to each other than 97%, the algorithm will sort out the best match and add the sequence to the correct ZOTU sequence. If you'd like to be more conservative, you can set this threshold to 99%, though this is not recommended by the authors.\n",
    "\n",
    "```{code-block} bash\n",
    "vsearch --usearch_global sequenceData/6-quality/combined.fasta --db sequenceData/8-final/zotus.fasta --strand plus --id 0.97 --otutabout sequenceData/8-final/zotutable.txt\n",
    "```\n",
    "\n",
    "````{admonition} Output\n",
    ":class: note, dropdown\n",
    "```\n",
    "vsearch v2.23.0_macos_aarch64, 16.0GB RAM, 8 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading file sequenceData/8-final/zotus.fasta 100%  \n",
    "420804 nt in 1345 seqs, min 310, max 316, avg 313\n",
    "Masking 100% \n",
    "Counting k-mers 100% \n",
    "Creating k-mer index 100% \n",
    "Searching 100%  \n",
    "Matching unique query sequences: 2195816 of 2230993 (98.42%)\n",
    "Writing OTU table (classic) 100%  \n",
    "```\n",
    "````\n",
    "\n",
    "The output printed to the Terminal window shows we managed to map 2,195,816 (98.42%) sequences to the ZOTU list, which were incorporated in our frequency table. The output file is a simple text file, which we can open in Excel. Below is a screenshot where I coloured the samples (column headers) blue and the sequence list (row names) yellow.\n",
    "\n",
    "```{figure} freqtable.png\n",
    ":name: Frequency table output\n",
    "\n",
    ": The frequency table output file in Excel, where the samples are coloured blue and the sequences are coloured yellow.\n",
    "```\n",
    "\n",
    "### 8.1 Summarise sequence counts\n",
    "\n",
    "As a final step, let's update the python script that produces a bar graph again, now including the final read count incorporated for each sample. This new python script takes in five user arguments, the same four as before, plus the frequency table file name. This final bar graph will allow us to get a great overview of what we have accomplished today.\n",
    "\n",
    "```{code-block} bash\n",
    "nano sequenceData/1-scripts/rawMergedTrimmedFilteredFinalStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    "#! /usr/bin/env python3\n",
    "\n",
    "## import modules\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## user arguments\n",
    "mergedPath = sys.argv[1]\n",
    "rawPath = sys.argv[2]\n",
    "trimmedPath = sys.argv[3]\n",
    "filteredPath = sys.argv[4]\n",
    "freqTable = sys.argv[5]\n",
    "\n",
    "\n",
    "## first, create a sample name list\n",
    "mergedFileList = os.listdir(mergedPath)\n",
    "sampleNameList = []\n",
    "for mergedFile in mergedFileList:\n",
    "    sampleName = mergedFile.split('_merged.fastq')[0]\n",
    "    if sampleName.startswith('COI_') or sampleName.startswith('NTC_'):\n",
    "        sampleNameList.append(sampleName)\n",
    "\n",
    "## count number of raw and merged sequences for each sample in sampleNameList\n",
    "rawSeqCount = {}\n",
    "mergedSeqCount = {}\n",
    "trimmedSeqCount = {}\n",
    "filteredSeqCount = {}\n",
    "for sample in sampleNameList:\n",
    "    with open(f'{mergedPath}{sample}_merged.fastq', 'r') as mergedFile:\n",
    "        x = len(mergedFile.readlines()) / 4\n",
    "        mergedSeqCount[sample] = int(x)\n",
    "    with open(f'{rawPath}{sample}_1.fastq', 'r') as rawFile:\n",
    "        y = len(rawFile.readlines()) / 4\n",
    "        rawSeqCount[sample] = int(y)\n",
    "    with open(f'{trimmedPath}{sample}_trimmed.fastq', 'r') as trimmedFile:\n",
    "        z = len(trimmedFile.readlines()) / 4\n",
    "        trimmedSeqCount[sample] = int(z)\n",
    "    with open(f'{filteredPath}{sample}_filtered.fastq', 'r') as filteredFile:\n",
    "        a = len(filteredFile.readlines()) / 4\n",
    "        filteredSeqCount[sample] = int(a)\n",
    "\n",
    "freqTableDF = pd.read_csv(freqTable, delimiter = '\\t', index_col = 0).transpose()\n",
    "freqTableDF['Final'] = freqTableDF.sum(axis = 1)\n",
    "columnsToDrop = freqTableDF.columns.difference(['Final'])\n",
    "freqTableDF.drop(columns = columnsToDrop, inplace = True)\n",
    "\n",
    "## create a dataframe from the dictionaries\n",
    "df = pd.DataFrame({'Sample': list(rawSeqCount.keys()), 'Raw': list(rawSeqCount.values()), 'Merged': list(mergedSeqCount.values()), 'Trimmed': list(trimmedSeqCount.values()), 'Filtered': list(filteredSeqCount.values())})\n",
    "\n",
    "## merge both data frames\n",
    "merged_df = df.merge(freqTableDF, left_on = ['Sample'], right_index = True, how = 'inner')\n",
    "\n",
    "## sort the dataframe by raw reads in descending order\n",
    "merged_df = merged_df.sort_values(by='Raw', ascending=False)\n",
    "\n",
    "## calculate the percentage of merged/raw and format it with 2 decimal places and the '%' symbol\n",
    "merged_df['Percentage'] = (merged_df['Final'] / merged_df['Raw'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "\n",
    "## create a horizontal bar plot using seaborn\n",
    "plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
    "\n",
    "## use seaborn's barplot\n",
    "ax = sns.barplot(x='Raw', y='Sample', data=merged_df, label='Raw', color='#BBC6C8')\n",
    "sns.barplot(x='Merged', y='Sample', data=merged_df, label='Merged', color='#469597')\n",
    "sns.barplot(x='Trimmed', y='Sample', data=merged_df, label='Trimmed', color='#DDBEAA')\n",
    "sns.barplot(x='Filtered', y='Sample', data=merged_df, label='Filtered', color='#806491')\n",
    "sns.barplot(x='Final', y='Sample', data=merged_df, label='Final', color='#2F70AF')\n",
    "\n",
    "## add labels and title\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Samples')\n",
    "plt.title('Horizontal bar graph of raw, merged, trimmed, and filtered reads (Sorted by Total in Reverse)')\n",
    "\n",
    "## add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add raw read count next to the bars\n",
    "for i, v in enumerate(merged_df['Percentage']):\n",
    "    ax.text(merged_df['Raw'].values[i] + 50, i, v, va='center', fontsize=10, color='black')\n",
    "\n",
    "## save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('raw_and_merged_and_trimmed_and_filtered_and_final_bargraph.png', dpi = 300)\n",
    "```\n",
    "\n",
    "Press `ctrl +x` to exit out of the editor, followed by `y` and `return`.\n",
    "\n",
    "```{code-block} bash\n",
    "chmod +x sequenceData/1-scripts/rawMergedTrimmedFilteredFinalStatistics.py\n",
    "```\n",
    "\n",
    "```{code-block} bash\n",
    "./sequenceData/1-scripts/rawMergedTrimmedFilteredFinalStatistics.py sequenceData/2-raw/ sequenceData/2-raw/unzipped/ sequenceData/4-demux/ sequenceData/5-filter/ sequenceData/8-final/zotutable.txt\n",
    "```\n",
    "\n",
    "```{figure} raw_and_merged_and_trimmed_and_filtered_and_final_bargraph.png\n",
    ":name: Raw and merged and trimmed and filtered and final read count\n",
    "\n",
    ": Read count of raw, merged, trimmed, filtered, and final frequency table for each sample.\n",
    "```\n",
    "\n",
    "This bar graph shows that for the final frequency table, we have kept around 80% of the raw reads for each sample, which is very good and due to the high quality data we started with (remember the multiQC output from before!). Again, we see a big difference between the actual samples and negative controls is observed.\n",
    "\n",
    "**That's it for today, see you tomorrow when we will assign a taxonomic ID to our ZOTU sequence list!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}